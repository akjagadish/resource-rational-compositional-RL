{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../plots/' )\n",
    "from model_comparison import return_model_comparison_metrics\n",
    "from load_human_behavior import load_behavior, load_percondition_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TASKS = 20\n",
    "NUM_TRIALS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule='add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, _, _, best = np.load(f'/notebooks/modelfits/simulated_data_preds/mean_tracker_compositional/stats_mean_tracker_compositional_simulated_compositional_{rule}_composed.npy')\n",
    "mtc_probs = (actions==best).mean(1).mean(1).mean(1)[:, 2, 0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(data, EXPERIMENT):\n",
    "    \n",
    "    NUM_SUBTASKS = 3 if EXPERIMENT == 'compositional' else 1\n",
    "    SUBTASK = 2 if EXPERIMENT == 'compositional' else 0\n",
    "    comp_data = data[(data['experiment']==EXPERIMENT)]\n",
    "    subjs = comp_data.index \n",
    "    subj_ids = np.repeat(data[data.experiment == EXPERIMENT].index, NUM_TASKS*NUM_TRIALS)\n",
    "    actions = np.stack([data.iloc[idx].actions[SUBTASK::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "    regrets = np.stack([data.iloc[idx].regrets[SUBTASK::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "    lin_regrets = np.stack([data.iloc[idx].regrets[0::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "    if EXPERIMENT == 'compositional':\n",
    "        lin_regrets = np.stack([data.iloc[idx].regrets[0::NUM_SUBTASKS].sum(1).repeat(NUM_TRIALS) for idx in list(subjs)]).reshape(-1)\n",
    "        per_regrets = np.stack([data.iloc[idx].regrets[1::NUM_SUBTASKS].sum(1).repeat(NUM_TRIALS) for idx in list(subjs)]).reshape(-1)\n",
    "        linper_regrets =  np.stack([data.iloc[idx].regrets[2::NUM_SUBTASKS].sum(1).repeat(NUM_TRIALS) for idx in list(subjs)]).reshape(-1)\n",
    "    rewards = np.stack([data.iloc[idx].rewards[SUBTASK::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "    trials =  np.arange(NUM_TRIALS).reshape(1,-1).repeat(NUM_TASKS*len(subjs), 0).reshape(-1)\n",
    "    trials = trials - trials.mean()\n",
    "    opt_arms = np.stack([data.iloc[idx].best_actions[SUBTASK::NUM_SUBTASKS] for idx in list(subjs)]).reshape(-1)\n",
    "    conditions = np.stack([.5 if data.iloc[idx].experiment == 'compositional' else -.5 for idx in list(subjs)]).reshape(-1).repeat(NUM_TASKS*NUM_TRIALS)\n",
    "    \n",
    "    datadict = {'subj_ids': subj_ids, 'trials': trials, 'interaction': trials*conditions, 'conditions': conditions, 'regrets': regrets, 'linregrets': lin_regrets, 'perregrets': per_regrets,  'linperregrets': linper_regrets,  'lin_interaction': lin_regrets*trials, 'per_interaction': per_regrets*trials, 'linper_interaction': per_regrets*lin_regrets*trials} if EXPERIMENT == 'compositional' else {'subj_ids': subj_ids, 'trials': trials, 'interaction': trials*conditions, 'conditions': conditions, 'regrets': regrets}\n",
    "    return pd.DataFrame(datadict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = 'changepoint'\n",
    "data = load_behavior(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_actions_dict, pooled_rewards_dict, \\\n",
    "pooled_regrets_dict, pooled_times_dict, pooled_rewards_tasks_dict, \\\n",
    "pooled_regrets_tasks_dict, pooled_times_tasks_dict, n_subjs = load_percondition_metrics(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c = make_dataframe(data, 'compositional')\n",
    "data_nc = make_dataframe(data, 'noncompositional')\n",
    "pooled_data = pd.concat((data_c, data_nc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=\"trials\", y=\"regrets\", style=\"conditions\", data=pooled_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test compring regrets on the first trial of curriculum condition with optimal policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9372095963823686 0.0 38.2473512956393 0.0\n"
     ]
    }
   ],
   "source": [
    "people_regrets_first_trial_compositional = pooled_data['regrets'][(pooled_data['trials']==-2)*(pooled_data['conditions']==0.5)].values\n",
    "optimal_regrets_first_trial_compositional = np.zeros_like(people_regrets_first_trial_compositional)\n",
    "rng = np.random.default_rng()\n",
    "# calculate the t-statistic and p-value\n",
    "t_stat, p_val = ttest_ind(people_regrets_first_trial_compositional, optimal_regrets_first_trial_compositional, alternative='two-sided', permutations=1000, random_state=rng)\n",
    "print(people_regrets_first_trial_compositional.mean(), optimal_regrets_first_trial_compositional.mean(), t_stat, p_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assert people_regrets_first_trial_compositional.shape[0] == optimal_regrets_first_trial_compositional.shape[0]\n",
    "DOF = people_regrets_first_trial_compositional.shape[0] + optimal_regrets_first_trial_compositional.shape[0] - 2\n",
    "\n",
    "DOF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test comparing regrets on the first trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9372095963823686 3.0961608949628676 -15.105262892304685 3.017970387948785e-50\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "TRIAL = -2\n",
    "CONDITION1 = 0.5 \n",
    "CONDITION2 = -0.5 \n",
    "group1 = pooled_data['regrets'][(pooled_data['trials']==TRIAL)*(pooled_data['conditions']==CONDITION1)].values\n",
    "group2 = pooled_data['regrets'][(pooled_data['trials']==TRIAL)*(pooled_data['conditions']==CONDITION2)].values\n",
    "\n",
    "# calculate the t-statistic and p-value\n",
    "t_stat, p_val = ttest_ind(group1, group2)\n",
    "print(group1.mean(), group2.mean(), t_stat, p_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assert group1.shape[0]==group2.shape[0]\n",
    "DOF = group1.shape[0] + group2.shape[0] - 2\n",
    "DOF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test comparing regrets on first trial of noncompositional vs last trial of the composoitional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.751975741073964 1.9372095963823686 -2.60474084115732 0.009226649418787423\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "TRIAL1 = 2\n",
    "TRIAL2 = -2\n",
    "CONDITION1 = -0.5 \n",
    "CONDITION2 = 0.5 \n",
    "group1 = pooled_data['regrets'][(pooled_data['trials']==TRIAL1)*(pooled_data['conditions']==CONDITION1)].values\n",
    "group2 = pooled_data['regrets'][(pooled_data['trials']==TRIAL2)*(pooled_data['conditions']==CONDITION2)].values\n",
    "\n",
    "# calculate the t-statistic and p-value\n",
    "t_stat, p_val = ttest_ind(group1, group2)\n",
    "print(group1.mean(), group2.mean(), t_stat, p_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assert group1.shape[0]==group2.shape[0]\n",
    "DOF = group1.shape[0] + group2.shape[0] - 2\n",
    "rDOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test probs of optimal choice on first trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp = data[(data['experiment']=='compositional')]\n",
    "data_noncomp = data[(data['experiment']=='noncompositional')]\n",
    "prob_opt_noncompositonal_first = np.stack([data_noncomp.iloc[subj].optimal_actions[:, 0].mean() for subj in range(n_subjs['noncompositional'])])\n",
    "prob_opt_compositional_first = np.stack([data_comp.iloc[subj].optimal_actions[2::3, 0].mean() for subj in range(n_subjs['compositional'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16813725490196077 0.007997682629025234 0.3371559633027522 0.01580875109975486 -9.346964932148962 1.4159796602016358e-17\n"
     ]
    }
   ],
   "source": [
    "group1 = prob_opt_noncompositonal_first\n",
    "group2 = prob_opt_compositional_first\n",
    "\n",
    "# calculate the t-statistic and p-value\n",
    "t_stat, p_val = ttest_ind(group1, group2)\n",
    "print(group1.mean(), group1.std()/np.sqrt(len(group1)-1), group2.mean(), group2.std()/np.sqrt(len(group2)-1), t_stat, p_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assert group1.shape[0]==group2.shape[0]\n",
    "DOF = group1.shape[0] + group2.shape[0] - 2\n",
    "DOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-tests between fitted poster regresion coefficients for regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal -0.042261004691545076 7.518334757833702e-05 -0.046412650682539774 5.037790040261795e-05 45.87392914144813 0.0\n",
      "corner_non_optimal -0.0606851852409295 7.662927565391976e-05 -0.03050848261508484 5.004231055962401e-05 -329.72070730506977 0.0\n",
      "phasic_non_optimal -0.025895727444285525 7.066226351048486e-05 -0.020289969592711052 4.6723088732594134e-05 -66.17393966053896 0.0\n",
      "neither 0.11372631675081867 9.796539075451798e-05 0.07471662454894867 6.36792009641809e-05 333.86452359780526 0.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.stats import ttest_ind\n",
    "rng = np.random.default_rng()\n",
    "with open(f'../modelfits/analysis_regrets/traces_{rule}.pkl', 'rb') as fp:\n",
    "    trace_first, list_of_errors = pickle.load(fp)\n",
    "for error in list_of_errors:\n",
    "    lin = trace_first[error].posterior.w_lin.values.reshape(-1) \n",
    "    per = trace_first[error].posterior.w_per.values.reshape(-1) \n",
    "    t_stat, p_val = ttest_ind(lin, per, permutations=1000, random_state=rng, alternative='two-sided')\n",
    "    print(error, lin.mean(), lin.std()/np.sqrt(len(lin)-1), per.mean(), per.std()/np.sqrt(len(per)-1), t_stat, p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-tests between fitted poster regresion coefficients for model regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal -0.28297567562383763 0.00018989075123746288 -0.16034837951673594 0.0001512966277775982 -505.0662121469511 0.0\n",
      "corner_non_optimal -0.3174292523681016 0.00022682627574045628 -0.3628807671465295 0.00021123361161017726 146.6408342936396 0.0\n",
      "phasic_non_optimal -0.2482791514848122 0.00019720895709069927 -0.2945940515801995 0.00017950213578811063 173.67931420700154 0.0\n",
      "neither 0.7048298343059726 0.00040800987429745754 0.6280295066016681 0.00036068063424325833 141.02785466031565 0.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np \n",
    "\n",
    "rng = np.random.default_rng()\n",
    "with open(f'../modelfits/analysis_regrets/traces_{rule}_rl3.pkl', 'rb') as fp:\n",
    "    trace_first, list_of_errors = pickle.load(fp)\n",
    "for error in list_of_errors:\n",
    "    lin = trace_first[error].posterior.w_lin.values.reshape(-1) \n",
    "    per = trace_first[error].posterior.w_per.values.reshape(-1) \n",
    "    t_stat, p_val = ttest_ind(lin, per, permutations=1000, random_state=rng, alternative='two-sided')\n",
    "    print(error, lin.mean(), lin.std()/np.sqrt(len(lin)-1), per.mean(), per.std()/np.sqrt(len(per)-1), t_stat, p_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test between fitted model simulations and people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "fit_subtasks='composed'\n",
    "n_participants = 92 if rule=='add' else 109\n",
    "changepoint = False if rule == 'add' else True\n",
    "full = True if fit_subtasks=='all' else False\n",
    "means, std_errors = [], []\n",
    "\n",
    "# people probs\n",
    "data_comp = data[(data['experiment']=='compositional')]\n",
    "data_noncomp = data[(data['experiment']=='noncompositional')]\n",
    "prob_opt_compositional_first = np.stack([data_comp.iloc[subj].optimal_actions[2::3, 0].mean() for subj in range(n_subjs['compositional'])])\n",
    "means.append(prob_opt_compositional_first.mean())\n",
    "std_errors.append(prob_opt_compositional_first.std()/np.sqrt(n_subjs['compositional']))\n",
    "\n",
    "# RL3\n",
    "actions, _, _, _, true_best_action = torch.load(f'/notebooks/scratch/RL3NeurIPS/data/RL3Fits/fitted_simulations/all_stats_changepoint={changepoint}_full={full}_entropyTruejagadish2022curriculum-v0_pertrial0.pth')\n",
    "sim_rl3_probs = (actions==true_best_action).to(torch.float)[:, :20]\n",
    "means.append(sim_rl3_probs.mean(1).mean(0)[10])\n",
    "std_errors.append(sim_rl3_probs.mean(1).std(0)[10]/np.sqrt(sim_rl3_probs.shape[0]))\n",
    "\n",
    "TASK_ID = 2\n",
    "if rule == 'changepoint':\n",
    "    'test'\n",
    "    actions, _, _, best = np.load(f'/notebooks/modelfits/simulated_data_preds/mean_tracker_compositional/stats_mean_tracker_compositional_simulated_compositional_{rule}_composed.npy')\n",
    "    mtc_probs = (actions==best).mean(1).mean(1).mean(1)[:, TASK_ID, 0]\n",
    "    means.append(mtc_probs.mean(0))\n",
    "    std_errors.append(mtc_probs.std(0)/np.sqrt(n_participants))\n",
    "else:\n",
    "    actions, _, _, best = np.load(f'/notebooks/modelfits/simulated_data_preds/mean_tracker_compositional/stats_mean_tracker_compositional_simulated_compositional_{rule}_composed.npy')\n",
    "    mtc_probs = (actions==best).squeeze().mean(1).mean(1)[:, TASK_ID, 0]\n",
    "    means.append(mtc_probs.mean(0))\n",
    "    std_errors.append(mtc_probs.std(0)/np.sqrt(n_participants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### people and RL3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3371559633027522 0.01580875109975486 0.29220182 0.01316687457017781 2.1850140358506973 0.033\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "group1 = prob_opt_compositional_first\n",
    "group2 = sim_rl3_probs[:, :, 10].mean(1).reshape(-1).numpy()\n",
    "\n",
    "# calculate the t-statistic and p-value\n",
    "t_stat, p_val = ttest_ind(group1, group2, permutations=1000, random_state=rng)#, alternative='two-sided')\n",
    "print(group1.mean(), group1.std()/np.sqrt(len(group1)-1), group2.mean(), group2.std()/np.sqrt(len(group2)-1), t_stat, p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### people and MTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3371559633027522 0.01580875109975486 0.2861811926605505 0.0068882763197996915 2.9560405199838407 0.004\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "group1 = prob_opt_compositional_first\n",
    "group2 = mtc_probs = (actions==best).mean(1).mean(1).mean(1)[:, TASK_ID, 0]\n",
    "\n",
    "# calculate the t-statistic and p-value\n",
    "t_stat, p_val = ttest_ind(group1, group2, permutations=1000, random_state=rng)#, alternative='two-sided')\n",
    "print(group1.mean(), group1.std()/np.sqrt(len(group1)-1), group2.mean(), group2.std()/np.sqrt(len(group2)-1), t_stat, p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.61270513, 4.7946584 , 4.01431059, 2.98412726, 1.00044593]),\n",
       " array([0.   , 0.   , 0.   , 0.002, 0.311]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_stat, p_val"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is there an effect of trials and condition?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y, X = dmatrices('regrets ~ trials + interaction + conditions', data=pooled_data, return_type='dataframe')\n",
    "mod = sm.OLS(y, X) \n",
    "res = mod.fit()   \n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mixed Linear Model Regression Results\n",
      "=========================================================\n",
      "Model:            MixedLM Dependent Variable: regrets    \n",
      "No. Observations: 21100   Method:             REML       \n",
      "No. Groups:       211     Scale:              4.7961     \n",
      "Min. group size:  100     Log-Likelihood:     -46718.9750\n",
      "Max. group size:  100     Converged:          Yes        \n",
      "Mean group size:  100.0                                  \n",
      "---------------------------------------------------------\n",
      "              Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------\n",
      "Intercept      1.943    0.045  43.135 0.000  1.855  2.031\n",
      "trials        -0.274    0.011 -25.671 0.000 -0.295 -0.253\n",
      "interaction    0.115    0.021   5.396 0.000  0.073  0.157\n",
      "conditions    -0.829    0.090  -9.196 0.000 -1.005 -0.652\n",
      "Group Var      0.380    0.019                            \n",
      "=========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "md = smf.mixedlm(\"regrets ~ trials + interaction + conditions\", data=pooled_data, groups=pooled_data[\"subj_ids\"])#, re_formula=\"~trials\")\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how does regret change with trial number: compositional vs noncompositional?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mixed Linear Model Regression Results\n",
      "=========================================================\n",
      "Model:            MixedLM Dependent Variable: regrets    \n",
      "No. Observations: 10900   Method:             REML       \n",
      "No. Groups:       109     Scale:              3.9275     \n",
      "Min. group size:  100     Log-Likelihood:     -23073.8327\n",
      "Max. group size:  100     Converged:          Yes        \n",
      "Mean group size:  100.0                                  \n",
      "---------------------------------------------------------\n",
      "              Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------\n",
      "Intercept      1.529    0.074  20.772 0.000  1.385  1.673\n",
      "trials        -0.216    0.013 -16.112 0.000 -0.243 -0.190\n",
      "Group Var      0.551    0.041                            \n",
      "=========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## compositional\n",
    "md = smf.mixedlm(\"regrets ~ trials\", data=data_c, groups=data_c[\"subj_ids\"]) #, re_formula=\"~trials\")\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mixed Linear Model Regression Results\n",
      "=========================================================\n",
      "Model:            MixedLM Dependent Variable: regrets    \n",
      "No. Observations: 10200   Method:             REML       \n",
      "No. Groups:       102     Scale:              5.7243     \n",
      "Min. group size:  100     Log-Likelihood:     -23451.4542\n",
      "Max. group size:  100     Converged:          Yes        \n",
      "Mean group size:  100.0                                  \n",
      "---------------------------------------------------------\n",
      "              Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------\n",
      "Intercept      2.357    0.050  47.258 0.000  2.260  2.455\n",
      "trials        -0.331    0.017 -19.783 0.000 -0.364 -0.299\n",
      "Group Var      0.197    0.015                            \n",
      "=========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## noncompositional\n",
    "md = smf.mixedlm(\"regrets ~ trials\", data=data_nc, groups=data_nc[\"subj_ids\"])#, re_formula=\"~trials\")\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how well does cum. linear and periodic regrets explain composite regrets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Mixed Linear Model Regression Results\n",
      "=====================================================================\n",
      "Model:                MixedLM    Dependent Variable:    linperregrets\n",
      "No. Observations:     9200       Method:                REML         \n",
      "No. Groups:           92         Scale:                 45.2267      \n",
      "Min. group size:      100        Log-Likelihood:        -30957.8524  \n",
      "Max. group size:      100        Converged:             Yes          \n",
      "Mean group size:      100.0                                          \n",
      "---------------------------------------------------------------------\n",
      "                            Coef.  Std.Err.   z   P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------\n",
      "Intercept                    5.923    0.784 7.553 0.000  4.386  7.460\n",
      "linregrets                   0.181    0.068 2.663 0.008  0.048  0.314\n",
      "perregrets                   0.241    0.046 5.257 0.000  0.151  0.331\n",
      "Group Var                   52.124    1.244                          \n",
      "Group x linregrets Cov      -3.009    0.092                          \n",
      "linregrets Var               0.366    0.010                          \n",
      "Group x perregrets Cov      -1.930    0.061                          \n",
      "linregrets x perregrets Cov  0.066    0.005                          \n",
      "perregrets Var               0.169    0.004                          \n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "md = smf.mixedlm(\"linperregrets ~ linregrets + perregrets\", data=data_c, groups=data_c[\"subj_ids\"], re_formula=\"~linregrets + perregrets\")\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how well does cum. linear and periodic regrets explain composite regrets as a function of trial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajagadish/.local/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/home/ajagadish/.local/lib/python3.8/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n",
      "  warnings.warn(\n",
      "/home/ajagadish/.local/lib/python3.8/site-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Mixed Linear Model Regression Results\n",
      "======================================================================\n",
      "Model:                 MixedLM     Dependent Variable:     regrets    \n",
      "No. Observations:      9200        Method:                 REML       \n",
      "No. Groups:            92          Scale:                  4.9203     \n",
      "Min. group size:       100         Log-Likelihood:         -20935.2723\n",
      "Max. group size:       100         Converged:              Yes        \n",
      "Mean group size:       100.0                                          \n",
      "----------------------------------------------------------------------\n",
      "                            Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "----------------------------------------------------------------------\n",
      "Intercept                    1.202    0.110 10.904 0.000  0.986  1.418\n",
      "linregrets                   0.034    0.052  0.650 0.516 -0.068  0.135\n",
      "perregrets                   0.044    0.052  0.851 0.395 -0.057  0.146\n",
      "lin_interaction             -0.006    0.003 -1.779 0.075 -0.012  0.001\n",
      "per_interaction             -0.013    0.002 -5.861 0.000 -0.017 -0.009\n",
      "Group Var                    0.662    0.039                           \n",
      "Group x linregrets Cov      -0.067    0.022                           \n",
      "linregrets Var               0.240                                    \n",
      "Group x perregrets Cov      -0.041    0.022                           \n",
      "linregrets x perregrets Cov  0.004                                    \n",
      "perregrets Var               0.244                                    \n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "md = smf.mixedlm(\"regrets ~ linregrets + perregrets + lin_interaction + per_interaction\", data=data_c, groups=data_c[\"subj_ids\"], re_formula=\"~linregrets + perregrets\")\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## probability of optimal chioce on base tasks -> probability of optimal choice on the first trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = 'add'\n",
    "data = load_behavior(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp = data[(data['experiment']=='compositional')]\n",
    "data_noncomp = data[(data['experiment']=='noncompositional')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjs = {}\n",
    "n_subjs['compositional'] = len(data_comp)\n",
    "n_subjs['noncompositional'] = len(data_noncomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_optimal = 0.6\n",
    "prob_opt_noncompositonal = np.stack([data_noncomp.iloc[subj].optimal_actions.mean(1) for subj in range(n_subjs['noncompositional'])])\n",
    "prob_opt_compositional = np.stack([data_comp.iloc[subj].optimal_actions[2::3].mean(1) for subj in range(n_subjs['compositional'])])\n",
    "prob_opt_noncompositonal_first = np.stack([data_noncomp.iloc[subj].optimal_actions[:, 0].mean() for subj in range(n_subjs['noncompositional'])])\n",
    "prob_opt_compositional_first = np.stack([data_comp.iloc[subj].optimal_actions[2::3, 0].mean() for subj in range(n_subjs['compositional'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_opt_compositional_linear_first = np.stack([data_comp.iloc[subj].optimal_actions[0::3, :].mean() for subj in range(n_subjs['compositional'])])\n",
    "prob_opt_compositional_periodic_first = np.stack([data_comp.iloc[subj].optimal_actions[1::3, :].mean() for subj in range(n_subjs['compositional'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_probabilities_dataframe(data, EXPERIMENT):\n",
    "    \n",
    "    data_comp = data[(data['experiment']==EXPERIMENT)]\n",
    "    subj_ids = data_comp.index\n",
    "    prob_opt_compositional_linear = np.stack([data_comp.iloc[subj].optimal_actions[0::3, :].mean() for subj in range(n_subjs['compositional'])])\n",
    "    prob_opt_compositional_periodic = np.stack([data_comp.iloc[subj].optimal_actions[1::3, :].mean() for subj in range(n_subjs['compositional'])])\n",
    "    #prob_opt_compositional = np.stack([data_comp.iloc[subj].optimal_actions[2::3].mean(1) for subj in range(n_subjs['compositional'])])\n",
    "    prob_opt_compositional_first = np.stack([data_comp.iloc[subj].optimal_actions[2::3, 0].mean() for subj in range(n_subjs['compositional'])])\n",
    "    \n",
    "    datadict = {'subj_ids': subj_ids, 'p_lin': prob_opt_compositional_linear, 'p_per': prob_opt_compositional_periodic, 'p_linper':prob_opt_compositional_first, 'p_lin_and_per': prob_opt_compositional_periodic*prob_opt_compositional_first}\n",
    "    return pd.DataFrame(datadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = return_probabilities_dataframe(data,'compositional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj_ids</th>\n",
       "      <th>p_lin</th>\n",
       "      <th>p_per</th>\n",
       "      <th>p_linper</th>\n",
       "      <th>p_lin_and_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>270</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>280</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>281</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>283</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>288</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subj_ids  p_lin  p_per  p_linper  p_lin_and_per\n",
       "0          0   0.39   0.24      0.25         0.0600\n",
       "1          1   0.50   0.23      0.15         0.0345\n",
       "2          2   0.46   0.25      0.60         0.1500\n",
       "3          3   0.60   0.22      0.35         0.0770\n",
       "4         12   0.47   0.30      0.45         0.1350\n",
       "..       ...    ...    ...       ...            ...\n",
       "87       270   0.18   0.18      0.50         0.0900\n",
       "88       280   0.65   0.21      0.25         0.0525\n",
       "89       281   0.42   0.25      0.20         0.0500\n",
       "90       283   0.50   0.20      0.35         0.0700\n",
       "91       288   0.29   0.21      0.20         0.0420\n",
       "\n",
       "[92 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj_ids</th>\n",
       "      <th>p_lin</th>\n",
       "      <th>p_per</th>\n",
       "      <th>p_linper</th>\n",
       "      <th>p_lin_and_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.1350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subj_ids  p_lin  p_per  p_linper  p_lin_and_per\n",
       "0         0   0.39   0.24      0.25         0.0600\n",
       "1         1   0.50   0.23      0.15         0.0345\n",
       "2         2   0.46   0.25      0.60         0.1500\n",
       "3         3   0.60   0.22      0.35         0.0770\n",
       "4        12   0.47   0.30      0.45         0.1350"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Mixed Linear Model Regression Results\n",
      "======================================================\n",
      "Model:            MixedLM Dependent Variable: p_linper\n",
      "No. Observations: 92      Method:             REML    \n",
      "No. Groups:       92      Scale:              0.0154  \n",
      "Min. group size:  1       Log-Likelihood:     26.2700 \n",
      "Max. group size:  1       Converged:          Yes     \n",
      "Mean group size:  1.0                                 \n",
      "------------------------------------------------------\n",
      "              Coef. Std.Err.   z   P>|z| [0.025 0.975]\n",
      "------------------------------------------------------\n",
      "Intercept     0.110    0.071 1.547 0.122 -0.029  0.249\n",
      "p_lin         0.491    0.073 6.727 0.000  0.348  0.634\n",
      "p_per         0.333    0.161 2.067 0.039  0.017  0.649\n",
      "Group Var     0.015                                   \n",
      "======================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "md = smf.mixedlm(\"p_linper ~ p_lin + p_per \", data=probs, groups=probs[\"subj_ids\"])\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Mixed Linear Model Regression Results\n",
      "=========================================================\n",
      "Model:             MixedLM  Dependent Variable:  p_linper\n",
      "No. Observations:  92       Method:              REML    \n",
      "No. Groups:        92       Scale:               0.0011  \n",
      "Min. group size:   1        Log-Likelihood:      141.0769\n",
      "Max. group size:   1        Converged:           Yes     \n",
      "Mean group size:   1.0                                   \n",
      "---------------------------------------------------------\n",
      "              Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------\n",
      "Intercept      0.349    0.022  15.668 0.000  0.306  0.393\n",
      "p_lin          0.042    0.032   1.299 0.194 -0.021  0.106\n",
      "p_per         -1.301    0.095 -13.716 0.000 -1.487 -1.115\n",
      "p_lin_and_per  3.480    0.105  33.299 0.000  3.275  3.685\n",
      "Group Var      0.001                                     \n",
      "=========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "md = smf.mixedlm(\"p_linper ~ p_lin + p_per + p_lin_and_per \", data=probs, groups=probs[\"subj_ids\"])\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## probit regression from total rewards in the subtasks to p(opt_1) in composed task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = 'add'\n",
    "data = load_behavior(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp = data[(data['experiment']=='compositional')]\n",
    "data_noncomp = data[(data['experiment']=='noncompositional')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjs = {}\n",
    "n_subjs['compositional'] = len(data_comp)\n",
    "n_subjs['noncompositional'] = len(data_noncomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_transform(x):\n",
    "    return (x-x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prob_regrets_dataframe(data, EXPERIMENT):\n",
    "    \n",
    "    data_comp = data[(data['experiment']==EXPERIMENT)]\n",
    "    subj_ids = data_comp.index\n",
    "    subj_ids = np.repeat(subj_ids, NUM_TASKS)\n",
    "    lin_regrets = np.stack([data_comp.iloc[idx].regrets[0::3].sum(1) for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    per_regrets = np.stack([data_comp.iloc[idx].regrets[1::3].sum(1) for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    prob_opt_compositional_first = np.stack([data_comp.iloc[subj].optimal_actions[2::3, 0] for subj in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    \n",
    "    regrets = {'regrets_lin': z_transform(lin_regrets), 'regrets_per': z_transform(per_regrets)}#, 'regrets_linper': z_transform(lin_regrets*per_regrets)}\n",
    "    probabilities = {'p_linper': prob_opt_compositional_first}\n",
    "    return pd.DataFrame(regrets), pd.DataFrame(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets, probabilities = return_prob_regrets_dataframe(data, 'compositional')\n",
    "regrets = sm.add_constant(regrets, prepend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.675266\n",
      "         Iterations 4\n",
      "Parameters:  regrets_lin   -0.085593\n",
      "regrets_per   -0.161677\n",
      "const          0.145438\n",
      "dtype: float64\n",
      "Marginal effects: \n",
      "       Probit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:               p_linper\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "===============================================================================\n",
      "                 dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "regrets_lin    -0.0332      0.011     -2.895      0.004      -0.056      -0.011\n",
      "regrets_per    -0.0627      0.011     -5.497      0.000      -0.085      -0.040\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "probit_mod = sm.Probit(probabilities, regrets)\n",
    "probit_res = probit_mod.fit()\n",
    "probit_margeff = probit_res.get_margeff()\n",
    "print(\"Parameters: \", probit_res.params)\n",
    "print(\"Marginal effects: \")\n",
    "print(probit_margeff.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>p_linper</td>     <th>  No. Observations:  </th>  <td>  1840</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>  <td>  1837</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 04 Mar 2023</td> <th>  Pseudo R-squ.:     </th>  <td>0.01654</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>21:09:34</td>     <th>  Log-Likelihood:    </th> <td> -1242.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -1263.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>8.459e-10</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_lin</th> <td>   -0.0856</td> <td>    0.030</td> <td>   -2.874</td> <td> 0.004</td> <td>   -0.144</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_per</th> <td>   -0.1617</td> <td>    0.030</td> <td>   -5.363</td> <td> 0.000</td> <td>   -0.221</td> <td>   -0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>    0.1454</td> <td>    0.030</td> <td>    4.926</td> <td> 0.000</td> <td>    0.088</td> <td>    0.203</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               p_linper   No. Observations:                 1840\n",
       "Model:                         Probit   Df Residuals:                     1837\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Sat, 04 Mar 2023   Pseudo R-squ.:                 0.01654\n",
       "Time:                        21:09:34   Log-Likelihood:                -1242.5\n",
       "converged:                       True   LL-Null:                       -1263.4\n",
       "Covariance Type:            nonrobust   LLR p-value:                 8.459e-10\n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "regrets_lin    -0.0856      0.030     -2.874      0.004      -0.144      -0.027\n",
       "regrets_per    -0.1617      0.030     -5.363      0.000      -0.221      -0.103\n",
       "const           0.1454      0.030      4.926      0.000       0.088       0.203\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prob_regrets_dataframe_mean(data, EXPERIMENT):\n",
    "    \n",
    "    data_comp = data[(data['experiment']==EXPERIMENT)]\n",
    "    subj_ids = data_comp.index\n",
    "    subj_ids = np.repeat(subj_ids, NUM_TASKS)\n",
    "    lin_regrets = np.stack([data_comp.iloc[idx].regrets[0::3].mean(1) for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    per_regrets = np.stack([data_comp.iloc[idx].regrets[1::3].mean(1) for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    prob_opt_compositional_first = np.stack([data_comp.iloc[subj].optimal_actions[2::3, 0] for subj in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    \n",
    "    regrets = {'regrets_lin': z_transform(lin_regrets), 'regrets_per': z_transform(per_regrets), 'regrets_linper': z_transform(lin_regrets*per_regrets)}\n",
    "    probabilities = {'p_linper': prob_opt_compositional_first}\n",
    "    return pd.DataFrame(regrets), pd.DataFrame(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets, probabilities = return_prob_regrets_dataframe_mean(data, 'compositional')\n",
    "regrets = sm.add_constant(regrets, prepend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.673099\n",
      "         Iterations 4\n",
      "Parameters:  regrets_lin      -0.205574\n",
      "regrets_per      -0.298009\n",
      "regrets_linper    0.208696\n",
      "const             0.146096\n",
      "dtype: float64\n",
      "Marginal effects: \n",
      "       Probit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:               p_linper\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "==================================================================================\n",
      "                    dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "regrets_lin       -0.0794      0.020     -4.011      0.000      -0.118      -0.041\n",
      "regrets_per       -0.1151      0.022     -5.321      0.000      -0.158      -0.073\n",
      "regrets_linper     0.0806      0.028      2.860      0.004       0.025       0.136\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "probit_mod = sm.Probit(probabilities, regrets)\n",
    "probit_res = probit_mod.fit()\n",
    "probit_margeff = probit_res.get_margeff()\n",
    "print(\"Parameters: \", probit_res.params)\n",
    "print(\"Marginal effects: \")\n",
    "print(probit_margeff.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>p_linper</td>     <th>  No. Observations:  </th>  <td>  1840</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>  <td>  1836</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 04 Mar 2023</td> <th>  Pseudo R-squ.:     </th>  <td>0.01969</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>21:09:44</td>     <th>  Log-Likelihood:    </th> <td> -1238.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -1263.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>8.992e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_lin</th>    <td>   -0.2056</td> <td>    0.052</td> <td>   -3.958</td> <td> 0.000</td> <td>   -0.307</td> <td>   -0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_per</th>    <td>   -0.2980</td> <td>    0.057</td> <td>   -5.199</td> <td> 0.000</td> <td>   -0.410</td> <td>   -0.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_linper</th> <td>    0.2087</td> <td>    0.073</td> <td>    2.840</td> <td> 0.005</td> <td>    0.065</td> <td>    0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>          <td>    0.1461</td> <td>    0.030</td> <td>    4.941</td> <td> 0.000</td> <td>    0.088</td> <td>    0.204</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               p_linper   No. Observations:                 1840\n",
       "Model:                         Probit   Df Residuals:                     1836\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Sat, 04 Mar 2023   Pseudo R-squ.:                 0.01969\n",
       "Time:                        21:09:44   Log-Likelihood:                -1238.5\n",
       "converged:                       True   LL-Null:                       -1263.4\n",
       "Covariance Type:            nonrobust   LLR p-value:                 8.992e-11\n",
       "==================================================================================\n",
       "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "regrets_lin       -0.2056      0.052     -3.958      0.000      -0.307      -0.104\n",
       "regrets_per       -0.2980      0.057     -5.199      0.000      -0.410      -0.186\n",
       "regrets_linper     0.2087      0.073      2.840      0.005       0.065       0.353\n",
       "const              0.1461      0.030      4.941      0.000       0.088       0.204\n",
       "==================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cum regrets to errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = 'add'\n",
    "data = load_behavior(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_transform(x):\n",
    "    return (x-x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_rewarding_arm(data_comp, task_id, n_subjs):\n",
    "    return np.stack([np.repeat(data_comp.iloc[subj].actions[data_comp.iloc[subj].rewards.max(axis=1).reshape(20,3)[:, task_id].max(1).repeat(15).reshape(60, 5) == data_comp.iloc[subj].rewards],5).reshape(20, 5) == data_comp.iloc[subj].actions[2::3] for subj in range(n_subjs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_regrets_probabilities(data, EXPERIMENT, error='corner'):\n",
    "    \n",
    "    # regrets\n",
    "    data_comp = data[(data['experiment']==EXPERIMENT)]\n",
    "    n_subjs = {}\n",
    "    n_subjs['compositional'] = len(data_comp)\n",
    "    #n_subjs['noncompositional'] = len(data_noncomp)\n",
    "    subj_ids = data_comp.index\n",
    "    subj_ids = np.repeat(subj_ids, NUM_TASKS)\n",
    "    lin_regrets = np.stack([data_comp.iloc[idx].regrets[0::3].sum(1) for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    per_regrets = np.stack([data_comp.iloc[idx].regrets[1::3].sum(1) for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    \n",
    "    # corner arms\n",
    "    corner_arms_people = np.stack([(data_comp.iloc[subj].actions[2::3]==0).astype('int')+(data_comp.iloc[subj].actions[2::3]==5).astype('int') for subj in range(n_subjs['compositional'])])\n",
    "    MIN_TIMES = 2 # out of 5\n",
    "    corner_arms_picked = (corner_arms_people.sum(2)>MIN_TIMES).reshape(-1).astype('int')\n",
    "    \n",
    "    # unique arms\n",
    "    unique_actions_people = np.stack([np.array([len(np.unique(row)) for row in data_comp.iloc[subj].actions[2::3]]) for subj in range(n_subjs['compositional'])])\n",
    "    MIN_TIMES = 2 # out of 5\n",
    "    unique_actions_picked = (unique_actions_people>MIN_TIMES).reshape(-1).astype('int')\n",
    "    \n",
    "    # same arms\n",
    "    same_actions_picked = (unique_actions_people==1).reshape(-1).astype('int')\n",
    "    \n",
    "    # most rewarding periodi\n",
    "    most_rewarding_periodic_people = most_rewarding_arm(data_comp, [1], n_subjs['compositional'])\n",
    "    most_rewarding_periodic = (most_rewarding_periodic_people.sum(2)>MIN_TIMES).reshape(-1).astype('int')\n",
    "    \n",
    "    # most rewarding periodi\n",
    "    most_rewarding_linear_people = most_rewarding_arm(data_comp, [0], n_subjs['compositional'])\n",
    "    most_rewarding_linear = (most_rewarding_linear_people.sum(2)>MIN_TIMES).reshape(-1).astype('int')\n",
    "    \n",
    "    # composed or non-composed\n",
    "    criterion = 0.6\n",
    "    number_opt_composed = np.stack([data_comp.iloc[subj].optimal_actions[2::3].sum(1)/5 for subj in range(n_subjs['compositional'])])\n",
    "    composed_first_trial = np.stack([data_comp.iloc[subj].optimal_actions[2::3, 0] for subj in range(n_subjs['compositional'])])\n",
    "    composed = (number_opt_composed>criterion).reshape(-1)\n",
    "    non_composed = (number_opt_composed<=criterion).reshape(-1)\n",
    "    non_composed_first_trial = (composed_first_trial==False).reshape(-1)\n",
    "    \n",
    "    condition = non_composed\n",
    "    if error=='corner':\n",
    "        behaviour = corner_arms_picked\n",
    "    elif error =='no_exploration':\n",
    "        behaviour = same_actions_picked\n",
    "    elif error == 'over_exploration':\n",
    "        behaviour = unique_actions_picked\n",
    "    elif error == 'most_rewarding_periodic':\n",
    "        behaviour=most_rewarding_periodic\n",
    "    elif error == 'most_rewarding_linear':\n",
    "        behaviour=most_rewarding_linear\n",
    "    elif error=='optimal':\n",
    "        behaviour = composed\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    regrets = {'regrets_lin': z_transform(lin_regrets)[condition], 'regrets_per': z_transform(per_regrets)[condition]}#, 'regrets_linper': z_transform(lin_regrets*per_regrets)}\n",
    "    probabilities = {error: behaviour[condition]}\n",
    "    return pd.DataFrame(regrets), pd.DataFrame(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.683945\n",
      "         Iterations 4\n",
      "Parameters:  regrets_lin   -0.040413\n",
      "regrets_per   -0.151441\n",
      "dtype: float64\n",
      "Marginal effects: \n",
      "       Probit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:         no_exploration\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "===============================================================================\n",
      "                 dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "regrets_lin    -0.0159      0.016     -1.005      0.315      -0.047       0.015\n",
      "regrets_per    -0.0596      0.015     -3.916      0.000      -0.089      -0.030\n",
      "===============================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>no_exploration</td>  <th>  No. Observations:  </th>  <td>   897</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>  <td>   895</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 13 Mar 2023</td> <th>  Pseudo R-squ.:     </th>  <td>-0.2757</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>18:12:39</td>     <th>  Log-Likelihood:    </th> <td> -613.50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -480.92</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 1.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_lin</th> <td>   -0.0404</td> <td>    0.040</td> <td>   -1.004</td> <td> 0.316</td> <td>   -0.119</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_per</th> <td>   -0.1514</td> <td>    0.040</td> <td>   -3.821</td> <td> 0.000</td> <td>   -0.229</td> <td>   -0.074</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:         no_exploration   No. Observations:                  897\n",
       "Model:                         Probit   Df Residuals:                      895\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Mon, 13 Mar 2023   Pseudo R-squ.:                 -0.2757\n",
       "Time:                        18:12:39   Log-Likelihood:                -613.50\n",
       "converged:                       True   LL-Null:                       -480.92\n",
       "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "regrets_lin    -0.0404      0.040     -1.004      0.316      -0.119       0.039\n",
       "regrets_per    -0.1514      0.040     -3.821      0.000      -0.229      -0.074\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = 'no_exploration'\n",
    "regrets, probabilities = return_regrets_probabilities(data, 'compositional', error)\n",
    "#regrets = sm.add_constant(regrets, prepend=False)\n",
    "\n",
    "probit_mod = sm.Probit(probabilities, regrets)\n",
    "probit_res = probit_mod.fit()\n",
    "probit_margeff = probit_res.get_margeff()\n",
    "print(\"Parameters: \", probit_res.params)\n",
    "print(\"Marginal effects: \")\n",
    "print(probit_margeff.summary())\n",
    "\n",
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.665699\n",
      "         Iterations 4\n",
      "Parameters:  regrets_lin   -0.239299\n",
      "regrets_per   -0.140592\n",
      "dtype: float64\n",
      "Marginal effects: \n",
      "       Probit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                 corner\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "===============================================================================\n",
      "                 dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "regrets_lin    -0.0915      0.015     -6.065      0.000      -0.121      -0.062\n",
      "regrets_per    -0.0537      0.015     -3.530      0.000      -0.084      -0.024\n",
      "===============================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>corner</td>      <th>  No. Observations:  </th>  <td>   897</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>  <td>   895</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 13 Mar 2023</td> <th>  Pseudo R-squ.:     </th> <td>-0.01972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>22:03:52</td>     <th>  Log-Likelihood:    </th> <td> -597.13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -585.58</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 1.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_lin</th> <td>   -0.2393</td> <td>    0.042</td> <td>   -5.737</td> <td> 0.000</td> <td>   -0.321</td> <td>   -0.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_per</th> <td>   -0.1406</td> <td>    0.041</td> <td>   -3.459</td> <td> 0.001</td> <td>   -0.220</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                 corner   No. Observations:                  897\n",
       "Model:                         Probit   Df Residuals:                      895\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Mon, 13 Mar 2023   Pseudo R-squ.:                -0.01972\n",
       "Time:                        22:03:52   Log-Likelihood:                -597.13\n",
       "converged:                       True   LL-Null:                       -585.58\n",
       "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "regrets_lin    -0.2393      0.042     -5.737      0.000      -0.321      -0.158\n",
       "regrets_per    -0.1406      0.041     -3.459      0.001      -0.220      -0.061\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = 'corner'\n",
    "regrets, probabilities = return_regrets_probabilities(data, 'compositional', error)\n",
    "#regrets = sm.add_constant(regrets, prepend=False)\n",
    "\n",
    "probit_mod = sm.Probit(probabilities, regrets)\n",
    "probit_res = probit_mod.fit()\n",
    "probit_margeff = probit_res.get_margeff()\n",
    "print(\"Parameters: \", probit_res.params)\n",
    "print(\"Marginal effects: \")\n",
    "print(probit_margeff.summary())\n",
    "\n",
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.680237\n",
      "         Iterations 4\n",
      "Parameters:  regrets_lin    0.105070\n",
      "regrets_per    0.150103\n",
      "dtype: float64\n",
      "Marginal effects: \n",
      "       Probit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:       over_exploration\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "===============================================================================\n",
      "                 dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "regrets_lin     0.0411      0.016      2.645      0.008       0.011       0.072\n",
      "regrets_per     0.0587      0.015      3.903      0.000       0.029       0.088\n",
      "===============================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>over_exploration</td> <th>  No. Observations:  </th>  <td>   897</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>  <td>   895</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 04 Mar 2023</td> <th>  Pseudo R-squ.:     </th> <td>-0.004257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>21:52:08</td>     <th>  Log-Likelihood:    </th> <td> -610.17</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -607.59</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 1.000</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_lin</th> <td>    0.1051</td> <td>    0.040</td> <td>    2.615</td> <td> 0.009</td> <td>    0.026</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_per</th> <td>    0.1501</td> <td>    0.039</td> <td>    3.810</td> <td> 0.000</td> <td>    0.073</td> <td>    0.227</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:       over_exploration   No. Observations:                  897\n",
       "Model:                         Probit   Df Residuals:                      895\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Sat, 04 Mar 2023   Pseudo R-squ.:               -0.004257\n",
       "Time:                        21:52:08   Log-Likelihood:                -610.17\n",
       "converged:                       True   LL-Null:                       -607.59\n",
       "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "regrets_lin     0.1051      0.040      2.615      0.009       0.026       0.184\n",
       "regrets_per     0.1501      0.039      3.810      0.000       0.073       0.227\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = 'over_exploration'\n",
    "regrets, probabilities = return_regrets_probabilities(data, 'compositional', error)\n",
    "#regrets = sm.add_constant(regrets, prepend=False)\n",
    "\n",
    "probit_mod = sm.Probit(probabilities, regrets)\n",
    "probit_res = probit_mod.fit()\n",
    "probit_margeff = probit_res.get_margeff()\n",
    "print(\"Parameters: \", probit_res.params)\n",
    "print(\"Marginal effects: \")\n",
    "print(probit_margeff.summary())\n",
    "\n",
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.685501\n",
      "         Iterations 4\n",
      "Parameters:  regrets_lin   -0.105617\n",
      "regrets_per   -0.093800\n",
      "dtype: float64\n",
      "Marginal effects: \n",
      "         Probit Marginal Effects          \n",
      "==========================================\n",
      "Dep. Variable:     most_rewarding_periodic\n",
      "Method:                               dydx\n",
      "At:                                overall\n",
      "===============================================================================\n",
      "                 dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "regrets_lin    -0.0416      0.016     -2.648      0.008      -0.072      -0.011\n",
      "regrets_per    -0.0370      0.015     -2.395      0.017      -0.067      -0.007\n",
      "===============================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>most_rewarding_periodic</td> <th>  No. Observations:  </th>  <td>   897</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>Probit</td>          <th>  Df Residuals:      </th>  <td>   895</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                    <td>MLE</td>           <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>               <td>Sat, 04 Mar 2023</td>     <th>  Pseudo R-squ.:     </th> <td>-0.07437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                   <td>21:52:09</td>         <th>  Log-Likelihood:    </th> <td> -614.89</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>                <td>True</td>           <th>  LL-Null:           </th> <td> -572.33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>        <td>nonrobust</td>        <th>  LLR p-value:       </th>  <td> 1.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_lin</th> <td>   -0.1056</td> <td>    0.040</td> <td>   -2.618</td> <td> 0.009</td> <td>   -0.185</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_per</th> <td>   -0.0938</td> <td>    0.040</td> <td>   -2.372</td> <td> 0.018</td> <td>   -0.171</td> <td>   -0.016</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                             Probit Regression Results                             \n",
       "===================================================================================\n",
       "Dep. Variable:     most_rewarding_periodic   No. Observations:                  897\n",
       "Model:                              Probit   Df Residuals:                      895\n",
       "Method:                                MLE   Df Model:                            1\n",
       "Date:                     Sat, 04 Mar 2023   Pseudo R-squ.:                -0.07437\n",
       "Time:                             21:52:09   Log-Likelihood:                -614.89\n",
       "converged:                            True   LL-Null:                       -572.33\n",
       "Covariance Type:                 nonrobust   LLR p-value:                     1.000\n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "regrets_lin    -0.1056      0.040     -2.618      0.009      -0.185      -0.027\n",
       "regrets_per    -0.0938      0.040     -2.372      0.018      -0.171      -0.016\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = 'most_rewarding_periodic'\n",
    "regrets, probabilities = return_regrets_probabilities(data, 'compositional', error)\n",
    "#regrets = sm.add_constant(regrets, prepend=False)\n",
    "\n",
    "probit_mod = sm.Probit(probabilities, regrets)\n",
    "probit_res = probit_mod.fit()\n",
    "probit_margeff = probit_res.get_margeff()\n",
    "print(\"Parameters: \", probit_res.params)\n",
    "print(\"Marginal effects: \")\n",
    "print(probit_margeff.summary())\n",
    "\n",
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.683626\n",
      "         Iterations 4\n",
      "Parameters:  regrets_lin   -0.114423\n",
      "regrets_per   -0.109011\n",
      "dtype: float64\n",
      "Marginal effects: \n",
      "        Probit Marginal Effects         \n",
      "========================================\n",
      "Dep. Variable:     most_rewarding_linear\n",
      "Method:                             dydx\n",
      "At:                              overall\n",
      "===============================================================================\n",
      "                 dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "regrets_lin    -0.0450      0.016     -2.858      0.004      -0.076      -0.014\n",
      "regrets_per    -0.0429      0.015     -2.774      0.006      -0.073      -0.013\n",
      "===============================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>most_rewarding_linear</td> <th>  No. Observations:  </th>  <td>   897</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>Probit</td>         <th>  Df Residuals:      </th>  <td>   895</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                   <td>MLE</td>          <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Sat, 04 Mar 2023</td>    <th>  Pseudo R-squ.:     </th>  <td>-0.3202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>21:52:18</td>        <th>  Log-Likelihood:    </th> <td> -613.21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>               <td>True</td>          <th>  LL-Null:           </th> <td> -464.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>nonrobust</td>       <th>  LLR p-value:       </th>  <td> 1.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_lin</th> <td>   -0.1144</td> <td>    0.041</td> <td>   -2.819</td> <td> 0.005</td> <td>   -0.194</td> <td>   -0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>regrets_per</th> <td>   -0.1090</td> <td>    0.040</td> <td>   -2.739</td> <td> 0.006</td> <td>   -0.187</td> <td>   -0.031</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            Probit Regression Results                            \n",
       "=================================================================================\n",
       "Dep. Variable:     most_rewarding_linear   No. Observations:                  897\n",
       "Model:                            Probit   Df Residuals:                      895\n",
       "Method:                              MLE   Df Model:                            1\n",
       "Date:                   Sat, 04 Mar 2023   Pseudo R-squ.:                 -0.3202\n",
       "Time:                           21:52:18   Log-Likelihood:                -613.21\n",
       "converged:                          True   LL-Null:                       -464.48\n",
       "Covariance Type:               nonrobust   LLR p-value:                     1.000\n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "regrets_lin    -0.1144      0.041     -2.819      0.005      -0.194      -0.035\n",
       "regrets_per    -0.1090      0.040     -2.739      0.006      -0.187      -0.031\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = 'most_rewarding_linear'\n",
    "regrets, probabilities = return_regrets_probabilities(data, 'compositional', error)\n",
    "#regrets = sm.add_constant(regrets, prepend=False)\n",
    "\n",
    "probit_mod = sm.Probit(probabilities, regrets)\n",
    "probit_res = probit_mod.fit()\n",
    "probit_margeff = probit_res.get_margeff()\n",
    "print(\"Parameters: \", probit_res.params)\n",
    "print(\"Marginal effects: \")\n",
    "print(probit_margeff.summary())\n",
    "\n",
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multinomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = 'add'\n",
    "data = load_behavior(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_transform(x):\n",
    "    return (x-x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_rewarding_arm(data_comp, task_id, n_subjs):\n",
    "    return np.stack([np.repeat(data_comp.iloc[subj].actions[data_comp.iloc[subj].rewards.max(axis=1).reshape(20,3)[:, task_id].max(1).repeat(15).reshape(60, 5) == data_comp.iloc[subj].rewards],5).reshape(20, 5) == data_comp.iloc[subj].actions[2::3] for subj in range(n_subjs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_multinomials(data, EXPERIMENT, error='corner'):\n",
    "    \n",
    "    # regrets\n",
    "    data_comp = data[(data['experiment']==EXPERIMENT)]\n",
    "    n_subjs = {}\n",
    "    n_subjs[EXPERIMENT] = len(data_comp)\n",
    "    #n_subjs['noncompositional'] = len(data_noncomp)\n",
    "    subj_ids = data_comp.index\n",
    "    subj_ids = np.repeat(subj_ids, NUM_TASKS)\n",
    "    lin_regrets = np.stack([data_comp.iloc[idx].regrets[0::3].sum(1) for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    per_regrets = np.stack([data_comp.iloc[idx].regrets[1::3].sum(1) for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    optimal = np.stack([data_comp.iloc[idx].optimal_actions[2::3] for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    optimal_first = np.stack([data_comp.iloc[idx].optimal_actions[2::3][:, 0].astype('int') for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    phase = np.stack([np.array(data.iloc[0].best_actions)[1::3] for idx in range(n_subjs['compositional'])]).reshape(-1)\n",
    "    even_phase = (phase+1)%2\n",
    "    odd_phase = ((phase+1)%2==0).astype('int')\n",
    "                     \n",
    "    # corner arms\n",
    "    corner_arms_people = np.stack([(data_comp.iloc[subj].actions[2::3]==0).astype('int')+(data_comp.iloc[subj].actions[2::3]==5).astype('int') for subj in range(n_subjs['compositional'])])\n",
    "    corner_arms_first = corner_arms_people[...,0].reshape(-1)\n",
    "    \n",
    "    #phasic arms\n",
    "    even_phasic_arms_people = np.stack([(data_comp.iloc[subj].actions[2::3]==0).astype('int')+(data_comp.iloc[subj].actions[2::3]==2)+(data_comp.iloc[subj].actions[2::3]==4).astype('int') for subj in range(n_subjs['compositional'])])\n",
    "    odd_phasic_arms_people = np.stack([(data_comp.iloc[subj].actions[2::3]==1).astype('int')+(data_comp.iloc[subj].actions[2::3]==3)+(data_comp.iloc[subj].actions[2::3]==5).astype('int') for subj in range(n_subjs['compositional'])])\n",
    "    even_phasic_arms_first = even_phasic_arms_people[...,0].reshape(-1)\n",
    "    odd_phasic_arms_first = odd_phasic_arms_people[...,0].reshape(-1)\n",
    "    \n",
    "    # most rewarding periodic\n",
    "    most_rewarding_periodic_people = most_rewarding_arm(data_comp, [1], n_subjs['compositional'])\n",
    "    most_rewarding_periodic_first = most_rewarding_periodic_people[..., 0].reshape(-1).astype('int')\n",
    "    \n",
    "    # most rewarding linear\n",
    "    most_rewarding_linear_people = most_rewarding_arm(data_comp, [0], n_subjs['compositional'])\n",
    "    most_rewarding_linear_first = most_rewarding_linear_people[...,0].reshape(-1).astype('int')\n",
    "    \n",
    "    return lin_regrets, per_regrets, corner_arms_first, optimal_first, even_phasic_arms_first, odd_phasic_arms_first, most_rewarding_periodic_first, most_rewarding_linear_first, phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regrets, per_regrets, corner, optimal, even_arms, odd_arms, mr_p, mr_l, phase = return_multinomials(data, 'compositional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_non_optimal = (optimal==0)*corner\n",
    "even_arms_non_optimal = (optimal==0)*even_arms\n",
    "odd_arms_non_optimal = (optimal==0)*odd_arms\n",
    "mr_p_non_optimal = (optimal==0)*mr_p\n",
    "mr_l_non_optimal = (optimal==0)*mr_l"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## corner arms multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_non_optimal = (optimal==0)*corner\n",
    "neither_corner_non_optimal = (optimal==0)*(corner==0)\n",
    "\n",
    "## multiple classes\n",
    "#corner_non_optimal[corner_non_optimal==1]=2\n",
    "# non_corner_classes = optimal+corner_non_optimal\n",
    "# non_corner_classes[non_corner_classes==0] = 3\n",
    "# non_corner_classes[non_corner_classes<3]=0\n",
    "# non_corner_classes[non_corner_classes==3]=1\n",
    "\n",
    "y_class = {}\n",
    "y_class['corner'] = corner_non_optimal\n",
    "y_class['optimal'] = optimal \n",
    "y_class['neither'] = neither_corner_non_optimal.astype('int')\n",
    "model = {}\n",
    "traces = {}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "((y_class['corner']+y_class['optimal']+ y_class['neither'])<1).sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(y_class['corner'], y_class['optimal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from theano.tensor.nnet.nnet import softmax\n",
    "from theano import tensor as tt\n",
    "import theano as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression(model_matrix):    \n",
    "    # load the data\n",
    "    x_lin = model_matrix['x_lin']\n",
    "    x_per = model_matrix['x_per']\n",
    "    y = model_matrix['y']\n",
    "    n_subj = 1\n",
    "\n",
    "    n = x_lin.shape\n",
    "    sample_kwargs = dict(draws=1000, random_seed=0, cores=64) #dict(draws=100, return_inferencedata=True, tune=100, init='advi+adapt_diag') \n",
    "\n",
    "    with pm.Model() as hier_model:\n",
    "\n",
    "        # set priors means\n",
    "        mu_1 = -0.20\n",
    "        mu_2 = -0.15\n",
    "        # set prior stds\n",
    "        sigma_1 = 1. \n",
    "        sigma_2 = 1.\n",
    "\n",
    "        w_1 = pm.Normal('w_lin',  mu=mu_1, sd=sigma_1, shape=n_subj)\n",
    "        w_2 = pm.Normal('w_per', mu=mu_2, sd=sigma_2, shape=n_subj)\n",
    "\n",
    "        rho = w_1 * x_lin + w_2 * x_per\n",
    "        p_hat = softmax(rho)\n",
    "\n",
    "        # Data likelihood\n",
    "        yl = pm.Categorical('yl', p=p_hat, observed=y)\n",
    "\n",
    "        # inference!\n",
    "        trace = pm.sample_smc(**sample_kwargs) \n",
    "    return hier_model, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = {}\n",
    "trace_ = {}\n",
    "regrets, optimal = return_regrets_probabilities(data, 'compositional', 'optimal')\n",
    "_, corner = return_regrets_probabilities(data, 'compositional', 'corner')\n",
    "optimal = optimal.values.astype('int')\n",
    "corner = corner.values.astype('int')\n",
    "corner_non_optimal = (optimal==0)*(corner==1)\n",
    "neither_corner_non_optimal = (optimal==0)*(corner==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing SMC sampler...\n",
      "Sampling 64 chains in 64 jobs\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.444\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.497\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.137\n",
      "Stage:   5 Beta: 0.467\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.138\n",
      "Stage:   5 Beta: 0.476\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.133\n",
      "Stage:   5 Beta: 0.452\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.045\n",
      "Stage:   4 Beta: 0.154\n",
      "Stage:   5 Beta: 0.527\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.135\n",
      "Stage:   5 Beta: 0.458\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.483\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.137\n",
      "Stage:   5 Beta: 0.454\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.122\n",
      "Stage:   5 Beta: 0.429\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.133\n",
      "Stage:   5 Beta: 0.438\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.448\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.122\n",
      "Stage:   5 Beta: 0.403\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.135\n",
      "Stage:   5 Beta: 0.454\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.137\n",
      "Stage:   5 Beta: 0.475\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.149\n",
      "Stage:   5 Beta: 0.497\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.137\n",
      "Stage:   5 Beta: 0.471\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.144\n",
      "Stage:   5 Beta: 0.488\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.446\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.136\n",
      "Stage:   5 Beta: 0.440\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.144\n",
      "Stage:   5 Beta: 0.496\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.147\n",
      "Stage:   5 Beta: 0.500\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.507\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.135\n",
      "Stage:   5 Beta: 0.468\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.451\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.495\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.138\n",
      "Stage:   5 Beta: 0.450\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.149\n",
      "Stage:   5 Beta: 0.499\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.142\n",
      "Stage:   5 Beta: 0.469\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.128\n",
      "Stage:   5 Beta: 0.429\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.441\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.478\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.128\n",
      "Stage:   5 Beta: 0.419\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.131\n",
      "Stage:   5 Beta: 0.458\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.444\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.014\n",
      "Stage:   3 Beta: 0.047\n",
      "Stage:   4 Beta: 0.161\n",
      "Stage:   5 Beta: 0.549\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.036\n",
      "Stage:   4 Beta: 0.118\n",
      "Stage:   5 Beta: 0.413\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.035\n",
      "Stage:   4 Beta: 0.118\n",
      "Stage:   5 Beta: 0.405\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.434\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.147\n",
      "Stage:   5 Beta: 0.505\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.137\n",
      "Stage:   5 Beta: 0.470\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.142\n",
      "Stage:   5 Beta: 0.498\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.431\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.135\n",
      "Stage:   5 Beta: 0.458\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.150\n",
      "Stage:   5 Beta: 0.514\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.131\n",
      "Stage:   5 Beta: 0.443\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.136\n",
      "Stage:   5 Beta: 0.480\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.125\n",
      "Stage:   5 Beta: 0.433\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.464\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.132\n",
      "Stage:   5 Beta: 0.458\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.130\n",
      "Stage:   5 Beta: 0.449\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.156\n",
      "Stage:   5 Beta: 0.529\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.146\n",
      "Stage:   5 Beta: 0.485\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.147\n",
      "Stage:   5 Beta: 0.475\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.136\n",
      "Stage:   5 Beta: 0.452\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.483\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.139\n",
      "Stage:   5 Beta: 0.478\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.136\n",
      "Stage:   5 Beta: 0.464\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.045\n",
      "Stage:   4 Beta: 0.152\n",
      "Stage:   5 Beta: 0.547\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.045\n",
      "Stage:   4 Beta: 0.144\n",
      "Stage:   5 Beta: 0.507\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.495\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.152\n",
      "Stage:   5 Beta: 0.530\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.474\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.494\n",
      "Stage:   6 Beta: 1.000\n"
     ]
    }
   ],
   "source": [
    "error = 'corner'\n",
    "#regrets, probabilities = return_regrets_probabilities(data, 'compositional', error)\n",
    "model_matrix = dict(#y=probabilities[error].values,\n",
    "                   y=corner_non_optimal,\n",
    "                   x_lin=regrets['regrets_lin'],\n",
    "                   x_per=regrets['regrets_per'])\n",
    "\n",
    "model_[error], trace_[error] = run_logistic_regression(model_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "error = 'corner'\n",
    "regrets, probabilities = return_regrets_probabilities(data, 'compositional', error)\n",
    "#regrets = sm.add_constant(regrets, prepend=False)\n",
    "\n",
    "probit_mod = sm.Probit(probabilities, regrets)\n",
    "probit_res = probit_mod.fit()\n",
    "probit_margeff = probit_res.get_margeff()\n",
    "print(\"Parameters: \", probit_res.params)\n",
    "print(\"Marginal effects: \")\n",
    "print(probit_margeff.summary())\n",
    "\n",
    "probit_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "error = 'corner'\n",
    "lin = pm.trace_to_dataframe(trace_[error])['w_lin__0'].values\n",
    "per = pm.trace_to_dataframe(trace_[error])['w_per__0'].values\n",
    "FONTSIZE = 13\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 4))#, dpi=100)\n",
    "#ax.hist(pm.trace_to_dataframe(traces[condition]), density=True,  bins=1000, label=['linear regrets', 'periodic regrets'])\n",
    "ax.hist(lin, density=True, bins=1000, label=['linear regrets'])\n",
    "ax.hist(per, density=True, bins=1000, label=['periodic regrets'])\n",
    "#plt.vlines(x=0., ymin=0, ymax=25, color='k')\n",
    "plt.xlim([-0.01, 1.])\n",
    "plt.title(f'Posterior distribution: {error}', fontsize=FONTSIZE) \n",
    "plt.xlabel('regression coefficients', fontsize=FONTSIZE) \n",
    "plt.ylabel('#samples', fontsize=FONTSIZE) \n",
    "plt.legend(frameon=False, loc='upper right')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "#f.savefig('/notebooks/figs/PLOS/ErrorAnalysis_{}.svg'.format(rule), bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing SMC sampler...\n",
      "Sampling 64 chains in 64 jobs\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.477\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.150\n",
      "Stage:   5 Beta: 0.513\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.145\n",
      "Stage:   5 Beta: 0.498\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.142\n",
      "Stage:   5 Beta: 0.491\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.146\n",
      "Stage:   5 Beta: 0.499\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.047\n",
      "Stage:   4 Beta: 0.161\n",
      "Stage:   5 Beta: 0.554\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.476\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.014\n",
      "Stage:   3 Beta: 0.046\n",
      "Stage:   4 Beta: 0.150\n",
      "Stage:   5 Beta: 0.511\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.146\n",
      "Stage:   5 Beta: 0.479\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.128\n",
      "Stage:   5 Beta: 0.454\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.476\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.490\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.442\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.143\n",
      "Stage:   5 Beta: 0.481\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.148\n",
      "Stage:   5 Beta: 0.508\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.046\n",
      "Stage:   4 Beta: 0.155\n",
      "Stage:   5 Beta: 0.523\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.486\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.045\n",
      "Stage:   4 Beta: 0.154\n",
      "Stage:   5 Beta: 0.517\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.468\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.145\n",
      "Stage:   5 Beta: 0.469\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.045\n",
      "Stage:   4 Beta: 0.157\n",
      "Stage:   5 Beta: 0.537\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.155\n",
      "Stage:   5 Beta: 0.524\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.147\n",
      "Stage:   5 Beta: 0.522\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.144\n",
      "Stage:   5 Beta: 0.492\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.478\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.149\n",
      "Stage:   5 Beta: 0.513\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.144\n",
      "Stage:   5 Beta: 0.468\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.046\n",
      "Stage:   4 Beta: 0.159\n",
      "Stage:   5 Beta: 0.533\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.045\n",
      "Stage:   4 Beta: 0.148\n",
      "Stage:   5 Beta: 0.497\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.138\n",
      "Stage:   5 Beta: 0.464\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.014\n",
      "Stage:   3 Beta: 0.047\n",
      "Stage:   4 Beta: 0.156\n",
      "Stage:   5 Beta: 0.513\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.157\n",
      "Stage:   5 Beta: 0.530\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.136\n",
      "Stage:   5 Beta: 0.445\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.147\n",
      "Stage:   5 Beta: 0.512\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.140\n",
      "Stage:   5 Beta: 0.470\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.014\n",
      "Stage:   3 Beta: 0.048\n",
      "Stage:   4 Beta: 0.169\n",
      "Stage:   5 Beta: 0.577\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.452\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.128\n",
      "Stage:   5 Beta: 0.429\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.476\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.047\n",
      "Stage:   4 Beta: 0.157\n",
      "Stage:   5 Beta: 0.544\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.151\n",
      "Stage:   5 Beta: 0.521\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.146\n",
      "Stage:   5 Beta: 0.512\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.137\n",
      "Stage:   5 Beta: 0.455\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.144\n",
      "Stage:   5 Beta: 0.492\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.045\n",
      "Stage:   4 Beta: 0.155\n",
      "Stage:   5 Beta: 0.521\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.144\n",
      "Stage:   5 Beta: 0.483\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.147\n",
      "Stage:   5 Beta: 0.519\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.138\n",
      "Stage:   5 Beta: 0.472\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.046\n",
      "Stage:   4 Beta: 0.149\n",
      "Stage:   5 Beta: 0.523\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.138\n",
      "Stage:   5 Beta: 0.469\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.145\n",
      "Stage:   5 Beta: 0.494\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.047\n",
      "Stage:   4 Beta: 0.165\n",
      "Stage:   5 Beta: 0.558\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.014\n",
      "Stage:   3 Beta: 0.047\n",
      "Stage:   4 Beta: 0.157\n",
      "Stage:   5 Beta: 0.520\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.159\n",
      "Stage:   5 Beta: 0.519\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.145\n",
      "Stage:   5 Beta: 0.483\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.046\n",
      "Stage:   4 Beta: 0.155\n",
      "Stage:   5 Beta: 0.521\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.146\n",
      "Stage:   5 Beta: 0.501\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.044\n",
      "Stage:   4 Beta: 0.149\n",
      "Stage:   5 Beta: 0.508\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.014\n",
      "Stage:   3 Beta: 0.049\n",
      "Stage:   4 Beta: 0.165\n",
      "Stage:   5 Beta: 0.585\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.014\n",
      "Stage:   3 Beta: 0.049\n",
      "Stage:   4 Beta: 0.160\n",
      "Stage:   5 Beta: 0.556\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.492\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.014\n",
      "Stage:   3 Beta: 0.048\n",
      "Stage:   4 Beta: 0.168\n",
      "Stage:   5 Beta: 0.584\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.152\n",
      "Stage:   5 Beta: 0.509\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.013\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.151\n",
      "Stage:   5 Beta: 0.521\n",
      "Stage:   6 Beta: 1.000\n"
     ]
    }
   ],
   "source": [
    "error = 'optimal'\n",
    "regrets, probabilities = return_regrets_probabilities(data, 'compositional', error)\n",
    "model_matrix = dict(y=probabilities[error].values,\n",
    "                   x_lin=regrets['regrets_lin'],\n",
    "                   x_per=regrets['regrets_per'])\n",
    "\n",
    "model_[error], trace_[error] = run_logistic_regression(model_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "error = 'optimal'\n",
    "lin = pm.trace_to_dataframe(trace_[error])['w_lin__0'].values\n",
    "per = pm.trace_to_dataframe(trace_[error])['w_per__0'].values\n",
    "FONTSIZE = 13\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 4))#, dpi=100)\n",
    "#ax.hist(pm.trace_to_dataframe(traces[condition]), density=True,  bins=1000, label=['linear regrets', 'periodic regrets'])\n",
    "ax.hist(lin, density=True, bins=1000, label=['linear regrets'])\n",
    "ax.hist(per, density=True, bins=1000, label=['periodic regrets'])\n",
    "#plt.vlines(x=0., ymin=0, ymax=175, color='k')\n",
    "plt.xlim([-0.01, 1.])\n",
    "plt.title(f'Posterior distribution: {error}', fontsize=FONTSIZE) \n",
    "plt.xlabel('regression coefficients', fontsize=FONTSIZE) \n",
    "plt.ylabel('#samples', fontsize=FONTSIZE) \n",
    "plt.legend(frameon=False, loc='upper right')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "#f.savefig('/notebooks/figs/PLOS/ErrorAnalysis_{}.svg'.format(rule), bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing SMC sampler...\n",
      "Sampling 64 chains in 64 jobs\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.126\n",
      "Stage:   5 Beta: 0.420\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.131\n",
      "Stage:   5 Beta: 0.448\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.132\n",
      "Stage:   5 Beta: 0.449\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.130\n",
      "Stage:   5 Beta: 0.450\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.122\n",
      "Stage:   5 Beta: 0.415\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.043\n",
      "Stage:   4 Beta: 0.146\n",
      "Stage:   5 Beta: 0.502\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.435\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.127\n",
      "Stage:   5 Beta: 0.424\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.128\n",
      "Stage:   5 Beta: 0.422\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.035\n",
      "Stage:   4 Beta: 0.114\n",
      "Stage:   5 Beta: 0.406\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.036\n",
      "Stage:   4 Beta: 0.121\n",
      "Stage:   5 Beta: 0.397\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.010\n",
      "Stage:   3 Beta: 0.036\n",
      "Stage:   4 Beta: 0.122\n",
      "Stage:   5 Beta: 0.428\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.036\n",
      "Stage:   4 Beta: 0.115\n",
      "Stage:   5 Beta: 0.378\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.133\n",
      "Stage:   5 Beta: 0.446\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.036\n",
      "Stage:   4 Beta: 0.121\n",
      "Stage:   5 Beta: 0.419\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.137\n",
      "Stage:   5 Beta: 0.453\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.130\n",
      "Stage:   5 Beta: 0.445\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.133\n",
      "Stage:   5 Beta: 0.450\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.036\n",
      "Stage:   4 Beta: 0.130\n",
      "Stage:   5 Beta: 0.429\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.124\n",
      "Stage:   5 Beta: 0.402\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.132\n",
      "Stage:   5 Beta: 0.452\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.454\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.136\n",
      "Stage:   5 Beta: 0.485\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.442\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.132\n",
      "Stage:   5 Beta: 0.440\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.131\n",
      "Stage:   5 Beta: 0.451\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.425\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.139\n",
      "Stage:   5 Beta: 0.479\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.133\n",
      "Stage:   5 Beta: 0.436\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.036\n",
      "Stage:   4 Beta: 0.118\n",
      "Stage:   5 Beta: 0.396\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.425\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.036\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.436\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.118\n",
      "Stage:   5 Beta: 0.390\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.035\n",
      "Stage:   4 Beta: 0.122\n",
      "Stage:   5 Beta: 0.427\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.124\n",
      "Stage:   5 Beta: 0.411\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.147\n",
      "Stage:   5 Beta: 0.494\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.010\n",
      "Stage:   3 Beta: 0.033\n",
      "Stage:   4 Beta: 0.109\n",
      "Stage:   5 Beta: 0.380\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.010\n",
      "Stage:   3 Beta: 0.033\n",
      "Stage:   4 Beta: 0.112\n",
      "Stage:   5 Beta: 0.379\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.035\n",
      "Stage:   4 Beta: 0.122\n",
      "Stage:   5 Beta: 0.414\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.133\n",
      "Stage:   5 Beta: 0.463\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.129\n",
      "Stage:   5 Beta: 0.447\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.471\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.035\n",
      "Stage:   4 Beta: 0.118\n",
      "Stage:   5 Beta: 0.387\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.128\n",
      "Stage:   5 Beta: 0.436\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.141\n",
      "Stage:   5 Beta: 0.473\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.126\n",
      "Stage:   5 Beta: 0.437\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.131\n",
      "Stage:   5 Beta: 0.462\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.010\n",
      "Stage:   3 Beta: 0.035\n",
      "Stage:   4 Beta: 0.118\n",
      "Stage:   5 Beta: 0.402\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.123\n",
      "Stage:   5 Beta: 0.432\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.035\n",
      "Stage:   4 Beta: 0.115\n",
      "Stage:   5 Beta: 0.398\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.036\n",
      "Stage:   4 Beta: 0.120\n",
      "Stage:   5 Beta: 0.414\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.147\n",
      "Stage:   5 Beta: 0.492\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.137\n",
      "Stage:   5 Beta: 0.459\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.039\n",
      "Stage:   4 Beta: 0.139\n",
      "Stage:   5 Beta: 0.448\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.126\n",
      "Stage:   5 Beta: 0.423\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.452\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.132\n",
      "Stage:   5 Beta: 0.457\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.125\n",
      "Stage:   5 Beta: 0.423\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.040\n",
      "Stage:   4 Beta: 0.136\n",
      "Stage:   5 Beta: 0.480\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.042\n",
      "Stage:   4 Beta: 0.139\n",
      "Stage:   5 Beta: 0.482\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.127\n",
      "Stage:   5 Beta: 0.446\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.004\n",
      "Stage:   2 Beta: 0.012\n",
      "Stage:   3 Beta: 0.041\n",
      "Stage:   4 Beta: 0.142\n",
      "Stage:   5 Beta: 0.494\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.037\n",
      "Stage:   4 Beta: 0.131\n",
      "Stage:   5 Beta: 0.443\n",
      "Stage:   6 Beta: 1.000\n",
      "Stage:   0 Beta: 0.001\n",
      "Stage:   1 Beta: 0.003\n",
      "Stage:   2 Beta: 0.011\n",
      "Stage:   3 Beta: 0.038\n",
      "Stage:   4 Beta: 0.134\n",
      "Stage:   5 Beta: 0.465\n",
      "Stage:   6 Beta: 1.000\n"
     ]
    }
   ],
   "source": [
    "error = 'neither'\n",
    "model_matrix = dict(y=neither_corner_non_optimal,\n",
    "                   x_lin=regrets['regrets_lin'],\n",
    "                   x_per=regrets['regrets_per'])\n",
    "\n",
    "model_[error], trace_[error] = run_logistic_regression(model_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "error = 'neither'\n",
    "lin = pm.trace_to_dataframe(trace_[error])['w_lin__0'].values\n",
    "per = pm.trace_to_dataframe(trace_[error])['w_per__0'].values\n",
    "FONTSIZE = 13\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 4))#, dpi=100)\n",
    "#ax.hist(pm.trace_to_dataframe(traces[condition]), density=True,  bins=1000, label=['linear regrets', 'periodic regrets'])\n",
    "ax.hist(lin, density=True, bins=1000, label=['linear regrets'])\n",
    "ax.hist(per, density=True, bins=1000, label=['periodic regrets'])\n",
    "#plt.vlines(x=0., ymin=0, ymax=175, color='k')\n",
    "plt.xlim([-0.01, 1.])\n",
    "plt.title(f'Posterior distribution: {error}', fontsize=FONTSIZE) \n",
    "plt.xlabel('regression coefficients', fontsize=FONTSIZE) \n",
    "plt.ylabel('#samples', fontsize=FONTSIZE) \n",
    "plt.legend(frameon=False, loc='upper right')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "#f.savefig('/notebooks/figs/PLOS/ErrorAnalysis_{}.svg'.format(rule), bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing SMC sampler...\n",
      "Sampling 64 chains in 64 jobs\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.026\n",
      "Stage:   8 Beta: 0.103\n",
      "Stage:   9 Beta: 0.325\n",
      "Stage:  10 Beta: 0.979\n",
      "Stage:  11 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.044\n",
      "Stage:   8 Beta: 0.158\n",
      "Stage:   9 Beta: 0.504\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.007\n",
      "Stage:   7 Beta: 0.026\n",
      "Stage:   8 Beta: 0.094\n",
      "Stage:   9 Beta: 0.325\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.046\n",
      "Stage:   8 Beta: 0.142\n",
      "Stage:   9 Beta: 0.476\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.040\n",
      "Stage:   8 Beta: 0.120\n",
      "Stage:   9 Beta: 0.443\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.043\n",
      "Stage:   8 Beta: 0.122\n",
      "Stage:   9 Beta: 0.356\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.016\n",
      "Stage:   7 Beta: 0.076\n",
      "Stage:   8 Beta: 0.256\n",
      "Stage:   9 Beta: 0.883\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.048\n",
      "Stage:   8 Beta: 0.178\n",
      "Stage:   9 Beta: 0.662\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.044\n",
      "Stage:   8 Beta: 0.158\n",
      "Stage:   9 Beta: 0.547\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.042\n",
      "Stage:   8 Beta: 0.136\n",
      "Stage:   9 Beta: 0.456\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.040\n",
      "Stage:   8 Beta: 0.147\n",
      "Stage:   9 Beta: 0.494\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.144\n",
      "Stage:   9 Beta: 0.608\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.040\n",
      "Stage:   8 Beta: 0.111\n",
      "Stage:   9 Beta: 0.376\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.046\n",
      "Stage:   8 Beta: 0.168\n",
      "Stage:   9 Beta: 0.640\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.047\n",
      "Stage:   8 Beta: 0.169\n",
      "Stage:   9 Beta: 0.584\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.032\n",
      "Stage:   8 Beta: 0.111\n",
      "Stage:   9 Beta: 0.369\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.054\n",
      "Stage:   8 Beta: 0.187\n",
      "Stage:   9 Beta: 0.598\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.007\n",
      "Stage:   7 Beta: 0.023\n",
      "Stage:   8 Beta: 0.079\n",
      "Stage:   9 Beta: 0.252\n",
      "Stage:  10 Beta: 0.989\n",
      "Stage:  11 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.026\n",
      "Stage:   8 Beta: 0.082\n",
      "Stage:   9 Beta: 0.314\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.052\n",
      "Stage:   8 Beta: 0.162\n",
      "Stage:   9 Beta: 0.492\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.129\n",
      "Stage:   9 Beta: 0.450\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.001\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.048\n",
      "Stage:   8 Beta: 0.151\n",
      "Stage:   9 Beta: 0.541\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.045\n",
      "Stage:   8 Beta: 0.186\n",
      "Stage:   9 Beta: 0.696\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.018\n",
      "Stage:   7 Beta: 0.054\n",
      "Stage:   8 Beta: 0.195\n",
      "Stage:   9 Beta: 0.632\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.048\n",
      "Stage:   8 Beta: 0.162\n",
      "Stage:   9 Beta: 0.542\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.119\n",
      "Stage:   9 Beta: 0.413\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.025\n",
      "Stage:   8 Beta: 0.089\n",
      "Stage:   9 Beta: 0.357\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.032\n",
      "Stage:   8 Beta: 0.105\n",
      "Stage:   9 Beta: 0.364\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.042\n",
      "Stage:   8 Beta: 0.164\n",
      "Stage:   9 Beta: 0.480\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.113\n",
      "Stage:   9 Beta: 0.378\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.032\n",
      "Stage:   8 Beta: 0.114\n",
      "Stage:   9 Beta: 0.335\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.029\n",
      "Stage:   8 Beta: 0.110\n",
      "Stage:   9 Beta: 0.353\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.007\n",
      "Stage:   7 Beta: 0.025\n",
      "Stage:   8 Beta: 0.085\n",
      "Stage:   9 Beta: 0.317\n",
      "Stage:  10 Beta: 0.908\n",
      "Stage:  11 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.046\n",
      "Stage:   8 Beta: 0.184\n",
      "Stage:   9 Beta: 0.609\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.036\n",
      "Stage:   8 Beta: 0.126\n",
      "Stage:   9 Beta: 0.463\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.042\n",
      "Stage:   8 Beta: 0.137\n",
      "Stage:   9 Beta: 0.472\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.038\n",
      "Stage:   8 Beta: 0.126\n",
      "Stage:   9 Beta: 0.368\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.121\n",
      "Stage:   9 Beta: 0.415\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.036\n",
      "Stage:   8 Beta: 0.120\n",
      "Stage:   9 Beta: 0.473\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.049\n",
      "Stage:   8 Beta: 0.159\n",
      "Stage:   9 Beta: 0.663\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.134\n",
      "Stage:   9 Beta: 0.411\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.137\n",
      "Stage:   9 Beta: 0.515\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.028\n",
      "Stage:   8 Beta: 0.101\n",
      "Stage:   9 Beta: 0.338\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.040\n",
      "Stage:   8 Beta: 0.136\n",
      "Stage:   9 Beta: 0.485\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.040\n",
      "Stage:   8 Beta: 0.132\n",
      "Stage:   9 Beta: 0.395\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.046\n",
      "Stage:   8 Beta: 0.161\n",
      "Stage:   9 Beta: 0.534\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.043\n",
      "Stage:   8 Beta: 0.147\n",
      "Stage:   9 Beta: 0.581\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.042\n",
      "Stage:   8 Beta: 0.126\n",
      "Stage:   9 Beta: 0.426\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.039\n",
      "Stage:   8 Beta: 0.131\n",
      "Stage:   9 Beta: 0.467\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.129\n",
      "Stage:   9 Beta: 0.392\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.016\n",
      "Stage:   7 Beta: 0.048\n",
      "Stage:   8 Beta: 0.190\n",
      "Stage:   9 Beta: 0.685\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.042\n",
      "Stage:   8 Beta: 0.149\n",
      "Stage:   9 Beta: 0.438\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.017\n",
      "Stage:   7 Beta: 0.058\n",
      "Stage:   8 Beta: 0.161\n",
      "Stage:   9 Beta: 0.462\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.049\n",
      "Stage:   8 Beta: 0.150\n",
      "Stage:   9 Beta: 0.497\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.123\n",
      "Stage:   9 Beta: 0.350\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.036\n",
      "Stage:   8 Beta: 0.132\n",
      "Stage:   9 Beta: 0.365\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.039\n",
      "Stage:   8 Beta: 0.150\n",
      "Stage:   9 Beta: 0.439\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.007\n",
      "Stage:   7 Beta: 0.030\n",
      "Stage:   8 Beta: 0.114\n",
      "Stage:   9 Beta: 0.343\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.043\n",
      "Stage:   8 Beta: 0.131\n",
      "Stage:   9 Beta: 0.409\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.045\n",
      "Stage:   8 Beta: 0.152\n",
      "Stage:   9 Beta: 0.527\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.137\n",
      "Stage:   9 Beta: 0.464\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.050\n",
      "Stage:   8 Beta: 0.199\n",
      "Stage:   9 Beta: 0.621\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.002\n",
      "Stage:   5 Beta: 0.006\n",
      "Stage:   6 Beta: 0.019\n",
      "Stage:   7 Beta: 0.068\n",
      "Stage:   8 Beta: 0.241\n",
      "Stage:   9 Beta: 0.750\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.035\n",
      "Stage:   8 Beta: 0.120\n",
      "Stage:   9 Beta: 0.443\n",
      "Stage:  10 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Sampling 64 chains in 64 jobs\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.116\n",
      "Stage:   9 Beta: 0.367\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.038\n",
      "Stage:   8 Beta: 0.137\n",
      "Stage:   9 Beta: 0.421\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.032\n",
      "Stage:   8 Beta: 0.108\n",
      "Stage:   9 Beta: 0.360\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.017\n",
      "Stage:   7 Beta: 0.050\n",
      "Stage:   8 Beta: 0.147\n",
      "Stage:   9 Beta: 0.494\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.001\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.046\n",
      "Stage:   8 Beta: 0.133\n",
      "Stage:   9 Beta: 0.447\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.047\n",
      "Stage:   8 Beta: 0.135\n",
      "Stage:   9 Beta: 0.412\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.062\n",
      "Stage:   8 Beta: 0.206\n",
      "Stage:   9 Beta: 0.711\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.043\n",
      "Stage:   8 Beta: 0.150\n",
      "Stage:   9 Beta: 0.569\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.046\n",
      "Stage:   8 Beta: 0.153\n",
      "Stage:   9 Beta: 0.543\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.047\n",
      "Stage:   8 Beta: 0.150\n",
      "Stage:   9 Beta: 0.555\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.163\n",
      "Stage:   9 Beta: 0.587\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.043\n",
      "Stage:   8 Beta: 0.158\n",
      "Stage:   9 Beta: 0.647\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.031\n",
      "Stage:   8 Beta: 0.088\n",
      "Stage:   9 Beta: 0.290\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.044\n",
      "Stage:   8 Beta: 0.166\n",
      "Stage:   9 Beta: 0.624\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.044\n",
      "Stage:   8 Beta: 0.163\n",
      "Stage:   9 Beta: 0.514\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.040\n",
      "Stage:   8 Beta: 0.134\n",
      "Stage:   9 Beta: 0.440\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.048\n",
      "Stage:   8 Beta: 0.159\n",
      "Stage:   9 Beta: 0.504\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.026\n",
      "Stage:   8 Beta: 0.088\n",
      "Stage:   9 Beta: 0.282\n",
      "Stage:  10 Beta: 0.926\n",
      "Stage:  11 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.030\n",
      "Stage:   8 Beta: 0.092\n",
      "Stage:   9 Beta: 0.375\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.052\n",
      "Stage:   8 Beta: 0.170\n",
      "Stage:   9 Beta: 0.528\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.039\n",
      "Stage:   8 Beta: 0.133\n",
      "Stage:   9 Beta: 0.512\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.001\n",
      "Stage:   4 Beta: 0.002\n",
      "Stage:   5 Beta: 0.006\n",
      "Stage:   6 Beta: 0.019\n",
      "Stage:   7 Beta: 0.060\n",
      "Stage:   8 Beta: 0.185\n",
      "Stage:   9 Beta: 0.724\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.045\n",
      "Stage:   8 Beta: 0.178\n",
      "Stage:   9 Beta: 0.662\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.002\n",
      "Stage:   5 Beta: 0.006\n",
      "Stage:   6 Beta: 0.019\n",
      "Stage:   7 Beta: 0.058\n",
      "Stage:   8 Beta: 0.195\n",
      "Stage:   9 Beta: 0.646\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.033\n",
      "Stage:   8 Beta: 0.117\n",
      "Stage:   9 Beta: 0.397\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.123\n",
      "Stage:   9 Beta: 0.394\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.023\n",
      "Stage:   8 Beta: 0.085\n",
      "Stage:   9 Beta: 0.347\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.031\n",
      "Stage:   8 Beta: 0.108\n",
      "Stage:   9 Beta: 0.396\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.160\n",
      "Stage:   9 Beta: 0.490\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.125\n",
      "Stage:   9 Beta: 0.448\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.027\n",
      "Stage:   8 Beta: 0.098\n",
      "Stage:   9 Beta: 0.280\n",
      "Stage:  10 Beta: 0.898\n",
      "Stage:  11 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.030\n",
      "Stage:   8 Beta: 0.118\n",
      "Stage:   9 Beta: 0.397\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.028\n",
      "Stage:   8 Beta: 0.093\n",
      "Stage:   9 Beta: 0.347\n",
      "Stage:  10 Beta: 0.992\n",
      "Stage:  11 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.018\n",
      "Stage:   7 Beta: 0.062\n",
      "Stage:   8 Beta: 0.251\n",
      "Stage:   9 Beta: 0.853\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.121\n",
      "Stage:   9 Beta: 0.415\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.044\n",
      "Stage:   8 Beta: 0.147\n",
      "Stage:   9 Beta: 0.458\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.045\n",
      "Stage:   8 Beta: 0.148\n",
      "Stage:   9 Beta: 0.423\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.126\n",
      "Stage:   9 Beta: 0.434\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.133\n",
      "Stage:   9 Beta: 0.498\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.049\n",
      "Stage:   8 Beta: 0.167\n",
      "Stage:   9 Beta: 0.697\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.156\n",
      "Stage:   9 Beta: 0.473\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.033\n",
      "Stage:   8 Beta: 0.119\n",
      "Stage:   9 Beta: 0.428\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.035\n",
      "Stage:   8 Beta: 0.123\n",
      "Stage:   9 Beta: 0.402\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.029\n",
      "Stage:   8 Beta: 0.106\n",
      "Stage:   9 Beta: 0.372\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.038\n",
      "Stage:   8 Beta: 0.118\n",
      "Stage:   9 Beta: 0.346\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.045\n",
      "Stage:   8 Beta: 0.148\n",
      "Stage:   9 Beta: 0.481\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.119\n",
      "Stage:   9 Beta: 0.457\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.045\n",
      "Stage:   8 Beta: 0.141\n",
      "Stage:   9 Beta: 0.482\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.032\n",
      "Stage:   8 Beta: 0.105\n",
      "Stage:   9 Beta: 0.371\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.017\n",
      "Stage:   7 Beta: 0.057\n",
      "Stage:   8 Beta: 0.186\n",
      "Stage:   9 Beta: 0.576\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.049\n",
      "Stage:   8 Beta: 0.186\n",
      "Stage:   9 Beta: 0.682\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.145\n",
      "Stage:   9 Beta: 0.435\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.047\n",
      "Stage:   8 Beta: 0.124\n",
      "Stage:   9 Beta: 0.373\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.006\n",
      "Stage:   6 Beta: 0.018\n",
      "Stage:   7 Beta: 0.065\n",
      "Stage:   8 Beta: 0.205\n",
      "Stage:   9 Beta: 0.597\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.038\n",
      "Stage:   8 Beta: 0.137\n",
      "Stage:   9 Beta: 0.399\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.036\n",
      "Stage:   8 Beta: 0.130\n",
      "Stage:   9 Beta: 0.393\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.038\n",
      "Stage:   8 Beta: 0.138\n",
      "Stage:   9 Beta: 0.407\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.007\n",
      "Stage:   7 Beta: 0.025\n",
      "Stage:   8 Beta: 0.099\n",
      "Stage:   9 Beta: 0.295\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.043\n",
      "Stage:   8 Beta: 0.133\n",
      "Stage:   9 Beta: 0.417\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.039\n",
      "Stage:   8 Beta: 0.143\n",
      "Stage:   9 Beta: 0.477\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.035\n",
      "Stage:   8 Beta: 0.135\n",
      "Stage:   9 Beta: 0.470\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.052\n",
      "Stage:   8 Beta: 0.204\n",
      "Stage:   9 Beta: 0.653\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.001\n",
      "Stage:   4 Beta: 0.002\n",
      "Stage:   5 Beta: 0.006\n",
      "Stage:   6 Beta: 0.021\n",
      "Stage:   7 Beta: 0.075\n",
      "Stage:   8 Beta: 0.248\n",
      "Stage:   9 Beta: 0.809\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.122\n",
      "Stage:   9 Beta: 0.460\n",
      "Stage:  10 Beta: 1.000\n",
      "Initializing SMC sampler...\n",
      "Sampling 64 chains in 64 jobs\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.029\n",
      "Stage:   8 Beta: 0.102\n",
      "Stage:   9 Beta: 0.332\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.036\n",
      "Stage:   8 Beta: 0.132\n",
      "Stage:   9 Beta: 0.409\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.027\n",
      "Stage:   8 Beta: 0.099\n",
      "Stage:   9 Beta: 0.351\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.042\n",
      "Stage:   8 Beta: 0.134\n",
      "Stage:   9 Beta: 0.433\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.001\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.052\n",
      "Stage:   8 Beta: 0.162\n",
      "Stage:   9 Beta: 0.579\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.045\n",
      "Stage:   8 Beta: 0.122\n",
      "Stage:   9 Beta: 0.401\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.074\n",
      "Stage:   8 Beta: 0.254\n",
      "Stage:   9 Beta: 0.827\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.042\n",
      "Stage:   8 Beta: 0.147\n",
      "Stage:   9 Beta: 0.581\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.017\n",
      "Stage:   7 Beta: 0.062\n",
      "Stage:   8 Beta: 0.206\n",
      "Stage:   9 Beta: 0.711\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.050\n",
      "Stage:   8 Beta: 0.154\n",
      "Stage:   9 Beta: 0.573\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.130\n",
      "Stage:   9 Beta: 0.430\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.040\n",
      "Stage:   8 Beta: 0.148\n",
      "Stage:   9 Beta: 0.582\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.106\n",
      "Stage:   9 Beta: 0.350\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.046\n",
      "Stage:   8 Beta: 0.169\n",
      "Stage:   9 Beta: 0.655\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.055\n",
      "Stage:   8 Beta: 0.207\n",
      "Stage:   9 Beta: 0.669\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.038\n",
      "Stage:   8 Beta: 0.134\n",
      "Stage:   9 Beta: 0.454\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.050\n",
      "Stage:   8 Beta: 0.149\n",
      "Stage:   9 Beta: 0.482\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.025\n",
      "Stage:   8 Beta: 0.087\n",
      "Stage:   9 Beta: 0.296\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.028\n",
      "Stage:   8 Beta: 0.092\n",
      "Stage:   9 Beta: 0.360\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.040\n",
      "Stage:   8 Beta: 0.132\n",
      "Stage:   9 Beta: 0.388\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.133\n",
      "Stage:   9 Beta: 0.483\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.001\n",
      "Stage:   4 Beta: 0.002\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.048\n",
      "Stage:   8 Beta: 0.147\n",
      "Stage:   9 Beta: 0.524\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.039\n",
      "Stage:   8 Beta: 0.154\n",
      "Stage:   9 Beta: 0.544\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.018\n",
      "Stage:   7 Beta: 0.061\n",
      "Stage:   8 Beta: 0.213\n",
      "Stage:   9 Beta: 0.687\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.044\n",
      "Stage:   8 Beta: 0.155\n",
      "Stage:   9 Beta: 0.530\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.111\n",
      "Stage:   9 Beta: 0.369\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.025\n",
      "Stage:   8 Beta: 0.093\n",
      "Stage:   9 Beta: 0.346\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.035\n",
      "Stage:   8 Beta: 0.112\n",
      "Stage:   9 Beta: 0.407\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.156\n",
      "Stage:   9 Beta: 0.473\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.117\n",
      "Stage:   9 Beta: 0.441\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.033\n",
      "Stage:   8 Beta: 0.123\n",
      "Stage:   9 Beta: 0.357\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.033\n",
      "Stage:   8 Beta: 0.125\n",
      "Stage:   9 Beta: 0.389\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.002\n",
      "Stage:   6 Beta: 0.007\n",
      "Stage:   7 Beta: 0.025\n",
      "Stage:   8 Beta: 0.083\n",
      "Stage:   9 Beta: 0.292\n",
      "Stage:  10 Beta: 0.853\n",
      "Stage:  11 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.043\n",
      "Stage:   8 Beta: 0.165\n",
      "Stage:   9 Beta: 0.567\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.033\n",
      "Stage:   8 Beta: 0.102\n",
      "Stage:   9 Beta: 0.376\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.043\n",
      "Stage:   8 Beta: 0.131\n",
      "Stage:   9 Beta: 0.496\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.017\n",
      "Stage:   7 Beta: 0.057\n",
      "Stage:   8 Beta: 0.201\n",
      "Stage:   9 Beta: 0.623\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.038\n",
      "Stage:   8 Beta: 0.132\n",
      "Stage:   9 Beta: 0.468\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.009\n",
      "Stage:   7 Beta: 0.035\n",
      "Stage:   8 Beta: 0.120\n",
      "Stage:   9 Beta: 0.487\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.053\n",
      "Stage:   8 Beta: 0.175\n",
      "Stage:   9 Beta: 0.717\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.134\n",
      "Stage:   9 Beta: 0.404\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.034\n",
      "Stage:   8 Beta: 0.122\n",
      "Stage:   9 Beta: 0.416\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.032\n",
      "Stage:   8 Beta: 0.115\n",
      "Stage:   9 Beta: 0.373\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.129\n",
      "Stage:   9 Beta: 0.465\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.016\n",
      "Stage:   7 Beta: 0.042\n",
      "Stage:   8 Beta: 0.130\n",
      "Stage:   9 Beta: 0.345\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.039\n",
      "Stage:   8 Beta: 0.131\n",
      "Stage:   9 Beta: 0.423\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.011\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.133\n",
      "Stage:   9 Beta: 0.512\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.046\n",
      "Stage:   8 Beta: 0.142\n",
      "Stage:   9 Beta: 0.483\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.043\n",
      "Stage:   8 Beta: 0.143\n",
      "Stage:   9 Beta: 0.505\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.041\n",
      "Stage:   8 Beta: 0.129\n",
      "Stage:   9 Beta: 0.406\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.006\n",
      "Stage:   6 Beta: 0.019\n",
      "Stage:   7 Beta: 0.059\n",
      "Stage:   8 Beta: 0.237\n",
      "Stage:   9 Beta: 0.815\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.036\n",
      "Stage:   8 Beta: 0.132\n",
      "Stage:   9 Beta: 0.424\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.046\n",
      "Stage:   8 Beta: 0.120\n",
      "Stage:   9 Beta: 0.362\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.014\n",
      "Stage:   7 Beta: 0.051\n",
      "Stage:   8 Beta: 0.156\n",
      "Stage:   9 Beta: 0.480\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.131\n",
      "Stage:   9 Beta: 0.423\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.144\n",
      "Stage:   9 Beta: 0.463\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.012\n",
      "Stage:   7 Beta: 0.037\n",
      "Stage:   8 Beta: 0.133\n",
      "Stage:   9 Beta: 0.403\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.008\n",
      "Stage:   7 Beta: 0.033\n",
      "Stage:   8 Beta: 0.125\n",
      "Stage:   9 Beta: 0.374\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.013\n",
      "Stage:   7 Beta: 0.040\n",
      "Stage:   8 Beta: 0.128\n",
      "Stage:   9 Beta: 0.435\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.035\n",
      "Stage:   8 Beta: 0.124\n",
      "Stage:   9 Beta: 0.431\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.004\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.047\n",
      "Stage:   8 Beta: 0.165\n",
      "Stage:   9 Beta: 0.567\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.005\n",
      "Stage:   6 Beta: 0.015\n",
      "Stage:   7 Beta: 0.060\n",
      "Stage:   8 Beta: 0.242\n",
      "Stage:   9 Beta: 0.750\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.002\n",
      "Stage:   5 Beta: 0.006\n",
      "Stage:   6 Beta: 0.021\n",
      "Stage:   7 Beta: 0.074\n",
      "Stage:   8 Beta: 0.239\n",
      "Stage:   9 Beta: 0.748\n",
      "Stage:  10 Beta: 1.000\n",
      "Stage:   0 Beta: 0.000\n",
      "Stage:   1 Beta: 0.000\n",
      "Stage:   2 Beta: 0.000\n",
      "Stage:   3 Beta: 0.000\n",
      "Stage:   4 Beta: 0.001\n",
      "Stage:   5 Beta: 0.003\n",
      "Stage:   6 Beta: 0.010\n",
      "Stage:   7 Beta: 0.032\n",
      "Stage:   8 Beta: 0.105\n",
      "Stage:   9 Beta: 0.379\n",
      "Stage:  10 Beta: 1.000\n"
     ]
    }
   ],
   "source": [
    "for condition in ['corner', 'optimal', 'neither']:\n",
    "#condition = 'neither'\n",
    "    model_matrix = dict(y=y_class[condition],\n",
    "                       x_lin=lin_regrets,\n",
    "                       x_per=per_regrets)\n",
    "\n",
    "    model[condition], traces[condition] = run_logistic_regression(model_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "condition = 'optimal'\n",
    "lin = pm.trace_to_dataframe(traces[condition])['w_lin__0'].values\n",
    "per = pm.trace_to_dataframe(traces[condition])['w_per__0'].values\n",
    "FONTSIZE = 13\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 4))#, dpi=100)\n",
    "#ax.hist(pm.trace_to_dataframe(traces[condition]), density=True,  bins=1000, label=['linear regrets', 'periodic regrets'])\n",
    "ax.hist(lin, density=True, bins=1000, label=['linear regrets'])\n",
    "ax.hist(per, density=True, bins=1000, label=['periodic regrets'])\n",
    "plt.vlines(x=0., ymin=0, ymax=175, color='k')\n",
    "plt.xlim([-0.01, .18])\n",
    "plt.title(f'Posterior distribution: {condition}', fontsize=FONTSIZE) \n",
    "plt.xlabel('regression coefficients', fontsize=FONTSIZE) \n",
    "plt.ylabel('#samples', fontsize=FONTSIZE) \n",
    "plt.legend(frameon=False, loc='upper right')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "#f.savefig('/notebooks/figs/PLOS/ErrorAnalysis_{}.svg'.format(rule), bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "condition = 'corner'\n",
    "lin = pm.trace_to_dataframe(traces[condition])['w_lin__0'].values\n",
    "per = pm.trace_to_dataframe(traces[condition])['w_per__0'].values\n",
    "FONTSIZE = 13\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 4))#, dpi=100)\n",
    "#ax.hist(pm.trace_to_dataframe(traces[condition]), density=True, bins=1000, label=['linear regrets', 'periodic regrets'])\n",
    "ax.hist(lin, density=True, bins=1000, label=['linear regrets'])\n",
    "ax.hist(per, density=True, bins=1000, label=['periodic regrets'])\n",
    "plt.vlines(x=0., ymin=0, ymax=175, color='k')\n",
    "plt.xlim([-0.01, .18])\n",
    "plt.title(f'Posterior distribution: {condition}', fontsize=FONTSIZE) \n",
    "plt.xlabel('regression coefficients', fontsize=FONTSIZE) \n",
    "plt.ylabel('#samples', fontsize=FONTSIZE) \n",
    "plt.legend(frameon=False, loc='upper right')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "#f.savefig('/notebooks/figs/PLOS/ErrorAnalysis_{}.svg'.format(rule), bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "condition = 'neither'\n",
    "lin = pm.trace_to_dataframe(traces[condition])['w_lin__0'].values\n",
    "per = pm.trace_to_dataframe(traces[condition])['w_per__0'].values\n",
    "FONTSIZE = 13\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 4))#, dpi=100)\n",
    "#ax.hist(pm.trace_to_dataframe(traces[condition]), density=True, bins=1000, label=['linear regrets', 'periodic regrets'])\n",
    "ax.hist(lin, density=True, bins=1000, label=['linear regrets'])\n",
    "ax.hist(per, density=True, bins=1000, label=['periodic regrets'])\n",
    "plt.vlines(x=0., ymin=0, ymax=175, color='k')\n",
    "plt.xlim([-0.01, .18])\n",
    "plt.title(f'Posterior distribution: {condition}', fontsize=FONTSIZE) \n",
    "plt.xlabel('regression coefficients', fontsize=FONTSIZE) \n",
    "plt.ylabel('#samples', fontsize=FONTSIZE) \n",
    "plt.legend(frameon=False, loc='upper right')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "#f.savefig('/notebooks/figs/PLOS/ErrorAnalysis_{}.svg'.format(rule), bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((lin_regrets, per_regrets)).T\n",
    "y = corner_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)#, penalty='l1')\n",
    "# define the model evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.559 (0.006)\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "# report the model performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02143006, -0.02894256],\n",
       "       [-0.02015541,  0.02114529],\n",
       "       [ 0.04158547,  0.00779727]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.discrete.discrete_model import MNLogit as mnlogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_non_optimal = (optimal==0)*corner\n",
    "corner_non_optimal[corner_non_optimal==1]=2\n",
    "corner_classes = optimal+corner_non_optimal\n",
    "corner_classes[corner_classes==0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((lin_regrets, per_regrets)).T\n",
    "y = corner_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.032819\n",
      "         Iterations 5\n",
      "Parameters:  [[-0.09376696 -0.04290051]\n",
      " [-0.01091176 -0.03657652]]\n",
      "Marginal effects: \n",
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 1840\n",
      "Model:                        MNLogit   Df Residuals:                     1836\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Thu, 09 Mar 2023   Pseudo R-squ.:                -0.03945\n",
      "Time:                        20:57:36   Log-Likelihood:                -1900.4\n",
      "converged:                       True   LL-Null:                       -1828.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
      "==============================================================================\n",
      "       y=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0938      0.013     -7.254      0.000      -0.119      -0.068\n",
      "x2            -0.0109      0.008     -1.391      0.164      -0.026       0.004\n",
      "------------------------------------------------------------------------------\n",
      "       y=3       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0429      0.012     -3.727      0.000      -0.065      -0.020\n",
      "x2            -0.0366      0.008     -4.582      0.000      -0.052      -0.021\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "mnlogit_model = mnlogit(y, X)\n",
    "mnlogit_model = mnlogit_model.fit()\n",
    "print(\"Parameters: \", mnlogit_model.params)\n",
    "print(\"Marginal effects: \")\n",
    "print(mnlogit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'multinomial',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (1000,)\n",
      "Counter({1: 334, 2: 334, 0: 332})\n"
     ]
    }
   ],
   "source": [
    "# test classification dataset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2, 0, 2, 2, 2, 0, 0, 1, 0, 2, 2, 0, 1, 1, 0, 1, 0, 2, 0,\n",
       "       2, 2, 0, 0, 2, 0, 1, 1, 2, 1, 0, 2, 0, 0, 0, 1, 2, 2, 2, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 0, 2, 2, 2, 1, 2, 1,\n",
       "       1, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 1, 2, 2, 2,\n",
       "       0, 1, 0, 2, 1, 0, 2, 1, 1, 2, 0, 1, 0, 0, 1, 1, 1, 0, 2, 2, 2, 2,\n",
       "       1, 2, 0, 2, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 0, 0, 1,\n",
       "       0, 0, 2, 2, 0, 0, 1, 1, 1, 1, 0, 2, 0, 1, 2, 0, 2, 1, 2, 0, 0, 1,\n",
       "       0, 0, 2, 2, 2, 2, 2, 1, 2, 0, 1, 2, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0,\n",
       "       2, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 0, 2, 1, 1, 0,\n",
       "       1, 1, 1, 1, 2, 1, 0, 2, 2, 1, 2, 1, 1, 2, 0, 0, 0, 2, 2, 2, 0, 2,\n",
       "       0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 0, 2, 2,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 0, 1, 1, 1, 1, 2,\n",
       "       1, 2, 1, 2, 0, 0, 0, 2, 1, 1, 0, 0, 1, 2, 2, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 2, 1, 2, 2, 2, 1, 0, 0, 1, 0, 0, 2, 1, 1, 0, 2, 2, 0, 2, 0, 2,\n",
       "       1, 0, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0, 1, 0, 1, 2, 1, 0, 2, 2, 0, 1,\n",
       "       2, 1, 2, 1, 1, 2, 0, 1, 0, 0, 1, 2, 2, 1, 2, 2, 2, 0, 1, 1, 0, 1,\n",
       "       2, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0,\n",
       "       1, 0, 1, 1, 1, 2, 1, 2, 0, 2, 1, 2, 0, 1, 1, 1, 1, 0, 0, 0, 1, 2,\n",
       "       2, 1, 0, 2, 2, 1, 0, 1, 1, 0, 1, 2, 0, 0, 2, 1, 0, 2, 0, 0, 1, 2,\n",
       "       0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 1, 0, 0, 1, 1, 2, 1, 2, 1, 0,\n",
       "       0, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 2, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 1, 1, 2, 2, 0, 1, 2, 2, 0, 2,\n",
       "       2, 0, 1, 0, 2, 2, 1, 1, 2, 2, 2, 0, 2, 2, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       2, 1, 1, 2, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 1,\n",
       "       0, 1, 2, 2, 2, 1, 1, 1, 1, 0, 0, 2, 0, 2, 0, 2, 1, 0, 2, 0, 1, 1,\n",
       "       0, 1, 2, 0, 1, 1, 1, 2, 2, 2, 2, 0, 1, 0, 0, 2, 0, 2, 2, 2, 2, 0,\n",
       "       0, 2, 0, 0, 2, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 0, 1, 1, 0, 1, 2, 1, 1, 0, 2, 2, 0, 2, 1, 0, 0, 2,\n",
       "       2, 2, 0, 2, 1, 0, 0, 0, 2, 1, 0, 1, 1, 0, 2, 0, 2, 2, 0, 2, 1, 2,\n",
       "       2, 2, 2, 0, 1, 2, 2, 2, 0, 1, 2, 2, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 2, 0, 2, 1, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 2, 1, 0, 2,\n",
       "       2, 2, 0, 0, 1, 0, 0, 2, 1, 0, 1, 2, 0, 0, 1, 0, 1, 1, 1, 2, 2, 1,\n",
       "       2, 1, 1, 0, 2, 1, 0, 1, 1, 2, 1, 1, 1, 0, 2, 0, 2, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 2, 2, 0,\n",
       "       2, 0, 0, 2, 2, 1, 0, 1, 1, 2, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 0,\n",
       "       0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 1, 2, 2, 0, 2, 0, 1,\n",
       "       1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 0, 2, 2, 0, 1, 2, 1, 2, 1, 2,\n",
       "       0, 1, 2, 1, 1, 0, 0, 0, 2, 2, 1, 1, 2, 2, 2, 2, 0, 1, 2, 0, 2, 2,\n",
       "       1, 2, 1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 1, 0,\n",
       "       2, 0, 2, 2, 1, 1, 0, 2, 1, 0, 0, 1, 1, 2, 1, 2, 2, 2, 0, 0, 2, 1,\n",
       "       2, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 0, 1, 2, 2, 1, 2, 2, 2, 2, 1,\n",
       "       2, 0, 0, 0, 1, 2, 0, 1, 1, 1, 2, 1, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1,\n",
       "       2, 2, 1, 2, 2, 2, 1, 2, 2, 0, 0, 1, 1, 0, 0, 2, 1, 0, 0, 2, 1, 1,\n",
       "       1, 2, 2, 0, 2, 1, 1, 2, 1, 1, 1, 2, 0, 2, 1, 2, 0, 1, 0, 2, 0, 2,\n",
       "       1, 1, 2, 1, 2, 2, 1, 0, 2, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rewards vs dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "rule = 'add'\n",
    "trials = np.arange(10,15)\n",
    "if rule == 'add':\n",
    "    _, load_rewards = torch.load('/notebooks/notebooks/temp/additive_curriculum.pth')  \n",
    "else:\n",
    "    _, load_rewards, _ = torch.load('/notebooks/notebooks/temp/changepoint_curriculum.pth')\n",
    "rewards = load_rewards[:, :, trials].sum(2).sum(1) # torch.stack(load_rewards)[1].sum(2).sum(1) if rule == 'add' else \n",
    "\n",
    "dls = torch.load('/notebooks/modelfits/rl3_stickiness/dls_{}.pth'.format(rule))\n",
    "dls[np.where(rewards==rewards.min())[0][0]] = np.NaN\n",
    "rewards[np.where(rewards==rewards.min())[0][0]] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl3 = pd.DataFrame.from_dict( {'rewards': rewards, 'dls': dls})\n",
    "\n",
    "md = smf.ols(\"rewards ~ dls\", data=rl3)\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3211821321362019, 0.30509192905112364)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mdf.rsquared), np.sqrt(mdf.rsquared_adj)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.corrcoef(rewards, dls)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## archive"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXPERIMENT = 'compositional'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "comp_data = data[(data['experiment'] == EXPERIMENT)]\n",
    "subjs = comp_data.index \n",
    "subj_ids = np.repeat(data[data.experiment == EXPERIMENT].index, NUM_TASKS*NUM_TRIALS)\n",
    "actions = np.stack([data.iloc[idx].actions[SUBTASK::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "regrets = np.stack([data.iloc[idx].regrets[SUBTASK::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "rewards = np.stack([data.iloc[idx].rewards[SUBTASK::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "trials =  np.arange(NUM_TRIALS).reshape(1,-1).repeat(NUM_TASKS).reshape(-1).repeat(len(subjs))\n",
    "trials = trials - trials.mean()\n",
    "opt_arms = np.stack([data.iloc[idx].best_actions[SUBTASK::NUM_SUBTASKS] for idx in list(subjs)]).reshape(-1)\n",
    "conditions = np.stack([.5 if data.iloc[idx].experiment == 'compositional' else -.5 for idx in list(subjs)]).reshape(-1).repeat(NUM_TASKS*NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit_data = pd.DataFrame({'subj_ids': subj_ids, 'trials': trials, 'interaction': trials*conditions, 'conditions': conditions, 'regrets': regrets})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXPERIMENT = 'noncompositional'\n",
    "NUM_SUBTASKS = 3 if EXPERIMENT == 'compositional' else 1\n",
    "SUBTASK = 2 if EXPERIMENT == 'compositional' else 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "comp_data = data[(data['experiment']==EXPERIMENT)]\n",
    "subjs = comp_data.index \n",
    "subj_ids = np.repeat(data[data.experiment == EXPERIMENT].index, NUM_TASKS*NUM_TRIALS)\n",
    "actions = np.stack([data.iloc[idx].actions[SUBTASK::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "regrets = np.stack([data.iloc[idx].regrets[SUBTASK::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "rewards = np.stack([data.iloc[idx].rewards[SUBTASK::NUM_SUBTASKS].reshape(-1) for idx in list(subjs)]).reshape(-1)\n",
    "trials =  np.arange(NUM_TRIALS).reshape(1,-1).repeat(NUM_TASKS).reshape(-1).repeat(len(subjs))\n",
    "trials = trials - trials.mean()\n",
    "opt_arms = np.stack([data.iloc[idx].best_actions[SUBTASK::NUM_SUBTASKS] for idx in list(subjs)]).reshape(-1)\n",
    "conditions = np.stack([.5 if data.iloc[idx].experiment == 'compositional' else -.5 for idx in list(subjs)]).reshape(-1).repeat(NUM_TASKS*NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit_data = pd.DataFrame({'subj_ids': subj_ids, 'trials': trials, 'interaction': trials*conditions, 'conditions': conditions, 'regrets': regrets})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
